---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Group Name's Group Project
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python (base)
    language: python
    name: base
---

```{python}
#| echo: false

'''
explanation: '| echo: false' means not showing source code, and 
            '| output: false' means not showing output of that code chunk (print, plt.show(), etc.)
And these should be placed in the first row of each code chunk.
'''

# import library used in this qmd file
import os
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt

from requests import get
import requests
from urllib.parse import urlparse
from functools import wraps
import zipfile
import shutil
```

```{python}
#| output: false

# Download the whole data file from remote repository:

# configuration parameter
GITHUB_ZIP_URL = "https://github.com/YYY677/fsds_505_not_found/archive/refs/heads/master.zip"
TARGET_FOLDER = "fsds_505_not_found-master/data"  # GitHub path
LOCAL_FOLDER = "./data"  # local file path

try:
    # check if the folder already exists
    if os.path.exists(LOCAL_FOLDER):
        print(f"The folder '{LOCAL_FOLDER}' already exists locally.")
    else:
        # download zip file from GitHub
        zip_path = "repo.zip"
        print("Downloading the repository ZIP file...")
        try:
            response = requests.get(GITHUB_ZIP_URL, timeout=30)
            response.raise_for_status()  # Raise an exception for HTTP errors
        except requests.exceptions.RequestException as e:
            print(f"Error downloading the ZIP file: {e}")
            exit(1)

        # write the ZIP file locally
        try:
            with open(zip_path, "wb") as f:
                f.write(response.content)
        except IOError as e:
            print(f"Error saving the ZIP file: {e}")
            exit(1)

        # extract the ZIP file
        print("Extracting the ZIP file...")
        try:
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(".")
        except zipfile.BadZipFile as e:
            print(f"Error extracting the ZIP file: {e}")
            os.remove(zip_path)
            exit(1)

        # move the target folder to the desired location
        print("Moving the target folder to the desired location...")
        try:
            shutil.move(TARGET_FOLDER, LOCAL_FOLDER)
        except FileNotFoundError as e:
            print(f"Error moving the folder: {e}")
            exit(1)
        except shutil.Error as e:
            print(f"Error during folder move: {e}")
            exit(1)

        # clean up temporary files
        print("Cleaning up temporary files...")
        try:
            os.remove(zip_path)
            shutil.rmtree("fsds_505_not_found-master")
        except Exception as e:
            print(f"Error during cleanup: {e}")
            # Not exiting here as the main task is complete

        print(f"The folder has been successfully downloaded to '{LOCAL_FOLDER}'.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")
```

```{python}
#| output: false

# This chunk is to down seperate files from remote repo:

def download_file(url, save_path):
    """
    Downloads a file from the given URL and saves it to the specified local path.
    """
    response = requests.get(url)
    if response.status_code == 200:
        with open(save_path, 'wb') as file:
            file.write(response.content)
        print(f"Download completed: {save_path}")
    else:
        print(f"Download failed with HTTP status code {response.status_code}, URL: {url}")

def check_and_download(url, local_path):
    """
    Checks if a file exists locally; if not, downloads it from the given URL.
    """
    if not os.path.exists(local_path):
        print(f"File does not exist. Downloading: {local_path}")
        download_file(url, local_path)
    else:
        print(f"File already exists: {local_path}")

# URLs and corresponding local save paths
url_csl = 'https://raw.githubusercontent.com/YYY677/fsds_505_not_found/master/harvard-cite-them-right.csl'
url_bib = 'https://raw.githubusercontent.com/YYY677/fsds_505_not_found/master/bio.bib'

# Specify local file paths
local_csl = "harvard-cite-them-right.csl"
local_bib = "bio.bib"

# Check and download the files
check_and_download(url_csl, local_csl)
check_and_download(url_bib, local_bib)
```

## Declaration of Authorship {.unnumbered .unlisted}

We, `505 not found`, pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date: 17/12/2024

Student Numbers: 24082251
                 24065306
                 24133780
                 24057655
                 24046535
                 
# Initial Research scope

## 1. Research Topic: 
 Exploring the Impact Profile of London's Neighbourhoods 

## 2. Methodology:
1. Correlation between Airbnb Rental Prices and Housing Prices
2. Multiple Linear Regression Analysis
3. K-Means Geodemographic Classification

## 3. Research objectives:
1. Investigate the Impact of Airbnb on Housing Prices and Availability:
2. Identify Affected Areas and Community Profiles
3. Summarize Policy Recommendations

# Response to Question

## 1. Who collected the InsideAirbnb data?

::: {.duedate}

Inside Airbnb data was collected by Murray Cox, a data activist and the project's founder, John Morris, the website designer and report producer, and Taylor Higgins, a master's student focusing on sustainable tourism at the Università degli Studi di Firenze.

:::

## 2. Why did they collect the InsideAirbnb data?

::: {.duedate}

The purpose of InsideAirbnb data is to provide data-driven insights into the impact of Airbnb on residential housing markets, thereby contributing to public discourse on the regulation and effects of short-term rental platforms in urban areas.

:::

## 3. How did they collect it?

::: {.duedate}

The data is collected by utilizing web scraping techniques such as self-made bots, inside Airbnb and AirDNA [@Pawlicz2021UNDERSTANDINGSR] [@Prentice2023AddressingDQ] to extract publicly available information from Airbnb’s website, focusing on various aspects of listings such as location, price, availability, and host details. This approach allows for the assembly of comprehensive datasets, which are then cleansed and organized to facilitate thorough analysis.

:::


## 4. How does the method of collection (Q3) impact the completeness and/or accuracy of the InsideAirbnb data? How well does it represent the process it seeks to study, and what wider issues does this raise?

::: {.duedate}

The data collection method used by InsideAirbnb raises data quality issues, mainly related to data incompleteness, reliance on website structure, and technical challenges. In terms of accuracy, data is automatically retrieved from the website, which possess the risk of capturing inaccurate or outdated information due to the dynamic nature of web content [@krotov2023big].

In addition, due to privacy measures, the geographic coordinates provided by Airbnb may not reflect the exact location of the listing, which adds a layer of inaccuracy. And as web scraping depends heavily on the structure of the Airbnb website. Changes to the website layout or measures to block scraping activities may disrupt data collection efforts, like Airbnb's anti-scrap measures including CAPTCHA or IP bans, pose additional challenges [@Prentice2023AddressingDQ]. This burdens data analysts by requiring them to constantly develop and maintain scraping scripts. 

In the discussion of the structure of InsideAirbnb data, it contains all aspects of the Airbnb market, including the distribution and characteristics of listings, pricing models, and the impact of Airbnb on the local housing market. And it is a relatively complete dataset and can assist with comprehensive analysis study.

Besides the accuracy concerns, the use of InsideAirbnb data raises technical, legal, and ethical issues.
Legally, as discussed in Sobel [@sobel2021new], scraping faces challenges in different jurisdictions, depending on how it intersects with privacy laws and terms of service agreements. This could affect the legality of the Inside Airbnb data collection process, especially if it violates Airbnb’s terms of service. Scraping also raises ethical issues, particularly regarding the consent of data subjects (Airbnb hosts and guests) whose information is collected without explicit permission. This raises significant privacy issues, as highlighted in the study by Xie and Karan [@xie2019consumers], where users’ awareness and concerns about how their data is used influence their privacy management behaviours.

:::

## 5. What ethical considerations does the use of the InsideAirbnb data raise? 

::: {.duedate}

The use of the InsideAirbnb database does raise several ethical considerations.

Firstly, there are issues of legal compliance. Web scraping can conflict with legal standards and ethical norms, particularly when data is collected without explicit consent, potentially leading to legal actions [@krotov2023big].

Secondly, privacy concerns for individuals must be addressed. Although the data might be publicly accessible, individuals typically do not anticipate their rental information being extensively aggregated and analyzed [@brenning2023web].

In many instances, data subjects (hosts and guests) are neither directly informed nor asked for consent when their data is scraped and analyzed. This presents a significant ethical dilemma: using their information without explicit permission, especially when such data might be utilized to draw conclusions or influence policies that could directly impact them.

Moreover, there is the issue of how policymaking might be influenced by the data. Since the scraped data can contain errors, issues with accuracy and potential misrepresentation may lead to misleading conclusions that could negatively affect Airbnb hosts, guests, and policy decisions.

Additionally, the misuse of data poses a significant ethical concern. When analyzing Inside Airbnb data, it is crucial to ensure that the data is not used for purposes unintended by the original data providers, such as market manipulation, unfair competition, or research that adversely impacts hosts and guests.

Lastly, transparency and accountability are crucial. Ethical research involving data scraping should clearly disclose its methodologies, the specific data collected, and how this data is utilized. Such transparency is especially important for accountability, particularly if the research has the potential to influence public opinion or policy [@brenning2023web].

:::



## 6. With reference to the InsideAirbnb data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and the types of properties that they list suggest about the nature of Airbnb lettings in London? 

### 6.1 Analysis of Hosts

```{python}
# read the latest airbnb listings dataset
df22 = pd.read_csv('data/listings.csv.gz', compression='gzip', low_memory=False)
```

```{python}
#| output: false

#expand count limitation of column display
pd.options.display.max_columns = 75 

# df22.info()
```

```{python}
# convert df(pandas) file to gdf(geopandas, which contains spatial column)
gdf = gpd.GeoDataFrame(df22, crs='EPSG:4326', geometry=gpd.points_from_xy(df22.longitude, df22.latitude)).to_crs(27700)

gdf = gdf[['host_since', 'host_id', 'host_listings_count', 'property_type', 'room_type', 'price', 'minimum_nights']]
# gdf.info()
```

```{python}
#| output: false

# This code chunk is about Wrangling Data

# Ensure 'host_since' is in datetime format
gdf['host_since'] = pd.to_datetime(gdf['host_since'], errors='coerce')
# Drop rows with NaT in 'host_since'
gdf = gdf.dropna(subset=['host_since'])

# Extract the year from 'host_since'
gdf['year'] = gdf['host_since'].dt.year.fillna(-1).astype(int)

# Handle missing values in 'host_listings_count'
gdf['host_listings_count'] = gdf['host_listings_count'].fillna(-1).astype(int)

# Clean 'price' column and convert it to float
gdf['price'] = gdf['price'].astype(str)
gdf['price'] = gdf['price'].str.replace('$','', regex=False).str.replace(',','', regex=False).astype(float).fillna(-1)

# Convert 'property_type' and 'room_type' to categorical data types
gdf['property_type'] = gdf['property_type'].astype('category')
gdf['room_type'] = gdf['room_type'].astype('category')

# gdf.head()
```

```{python}
#| output: false

gdf.info()
gdf['year'].min()
```

#### 6.1.1 Distribution of the Number of Listings per Host

```{python}
#| output: false

# This code chunk is to look at 'Distribution of the Number of Listings per Host'.

# Group by 'host_id' to count the number of listings per host
host_counts = gdf.groupby('host_id').size()

# Define bins and labels for the grouping
bins = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, float('inf')]  # 分组区间
labels = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10+']  # 分组标签

# Group the host counts into bins
host_counts_bins = pd.cut(host_counts, bins=bins, labels=labels, right=True)

# Count total listings per group (number * number of listings per host)
listings_per_group = host_counts.groupby(host_counts_bins).apply(lambda x: (x * x.index.to_series().value_counts()).sum())

# Calculate total number of listings
total_listings = listings_per_group.sum()
percentages = (listings_per_group / total_listings) * 100

# Plotting the bar chart
plt.figure(figsize=(8, 6))
bars = plt.bar(listings_per_group.index, listings_per_group.values, color='skyblue')

# Add labels with counts and percentages
for bar, count, pct in zip(bars, listings_per_group.values, percentages):
    plt.text(
        bar.get_x() + bar.get_width() / 2,  # x 坐标
        bar.get_height() + 0.5,  # y 坐标
        f'{count:.0f}\n({pct:.1f}%)',  # 标签内容
        ha='center', va='bottom', fontsize=10
    )

# Set the title and axis labels
plt.title('Distribution of Listings by Listings per Host', fontsize=16)
plt.xlabel('Listings per Host (Grouped)', fontsize=14)
plt.ylabel('Total Listings', fontsize=14)

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Display the plot
plt.tight_layout()
plt.show()
```

In London, only 47.8% (45,932 listings) are owned by single-listing hosts, while the remaining 52.2% are held by multi-listing hosts.  
Notably, hosts with 10 or more listings account for 20.8% (20,038 listings) of the total.

**conclusion:**

1. Prevalence of multi-listing hosts: more than half of all listings owned by multi-listing hosts, indicating that multi-listing is common in london.
2. Professional landlords: hosts who owned 10+ listings owned more than one fifth listings, suggesting a significant presence of professional landlords in the market.

#### 6.1.2 Changes in the number of landlords and renters over the years

```{python}
# --- Count the number of new host IDs added each year ---
new_hosts_per_year = gdf.drop_duplicates('host_id').groupby('year').size()

# --- Count the cumulative number of host IDs each year ---
year_range = range(int(gdf['year'].min()), int(gdf['year'].max()) + 1)
id_per_year = [gdf[gdf['year'] <= year].shape[0] for year in year_range]

# Create a DataFrame to display results
result_df = pd.DataFrame({
    'Year': year_range,
    'New Hosts': new_hosts_per_year.reindex(year_range, fill_value=0),
    'Total Listings Count': id_per_year
})

# Plotting the data
fig, ax1 = plt.subplots(figsize=(20, 12))  # Greatly increased figure size

# --- Plot the number of new hosts added each year (bar chart) ---
bars = ax1.bar(result_df['Year'], result_df['New Hosts'], color='#6baed6', label='New Hosts per Year', alpha=0.8)
ax1.set_xlabel('Year', fontsize=24, fontweight='bold')  # Significantly larger font size
ax1.set_ylabel('New Hosts', color='#6baed6', fontsize=24, fontweight='bold')
ax1.tick_params(axis='y', labelcolor='#6baed6', labelsize=22)
ax1.tick_params(axis='x', labelsize=22)

# Annotate bar heights with values
for bar in bars:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width() / 2, height, f'{int(height)}', 
             ha='center', va='bottom', fontsize=18, color='#6baed6')  # Slightly smaller font for annotations

# --- Add a second y-axis for cumulative listings (line chart) ---
ax2 = ax1.twinx()
lines = ax2.plot(result_df['Year'], result_df['Total Listings Count'], color='#fd8d3c', marker='o', markersize=10, label='Total Listings Count')
ax2.set_ylabel('Listings Count', color='#e6550d', fontsize=24, fontweight='bold')
ax2.tick_params(axis='y', labelcolor='#e6550d', labelsize=22)

# Annotate line chart points with values
for x, y in zip(result_df['Year'], result_df['Total Listings Count']):
    ax2.text(x, y + 500, f'{int(y)}', ha='center', va='bottom', fontsize=18, color='#e6550d')  # Slightly smaller font for annotations

# --- Set x-axis ticks to display years as integers ---
plt.xticks(ticks=result_df['Year'], labels=result_df['Year'], rotation=45, fontsize=22)

# --- Add a legend ---
fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.95), fontsize=20)

# --- Set the title of the plot ---
plt.title('Yearly New Hosts and Listings on Airbnb in London', fontsize=30, fontweight='bold')

# --- Display the plot ---
plt.tight_layout()
plt.show()
```

* Based on Airbnb's dataset for London, the whole story started in 2008 and the growth in hosts and listings was slow between 2008 and 2010, **accelerating sharply from 2011 to 2016**. The peak occurred in 2015, with 8,939 new hosts, while 2014 and 2016 saw increases of over 7,000 hosts each. By 2016, total listings neared 50,000. 

* However, **growth slowed in subsequent years**, with 2020 and 2021 adding only around 1,600 hosts and 3,000 listings annually—nearly half the growth seen in 2019—largely due to the pandemic’s impact on the rental market. From 2022 to 2024, post-pandemic recovery is evident, but growth remains far below peak levels.

### 6.2 Analysis of property
#### 6.2.1 Distribution of room types of property

```{python}
#| output: false

# Count the number of each property type
property_counts = gdf['property_type'].value_counts()

# Filter out the categories with frequency greater than 200
filtered_property_counts = property_counts[property_counts > 200]

# Calculate the percentage for each category
total_count = property_counts.sum()  # 总的数量
filtered_percentage = (filtered_property_counts / total_count) * 100

# Draw a bar chart
plt.figure(figsize=(10, 6))
bars = filtered_property_counts.plot(kind='bar', color='skyblue')

# Add title and tag
plt.title('Distribution of Property Types (Count > 200)')
plt.xlabel('Property Type')
plt.ylabel('Count')

for bar, count, percentage in zip(bars.patches, filtered_property_counts, filtered_percentage):
    height = bar.get_height()
    # Show count at the top and percentage at the bottom
    plt.text(bar.get_x() + bar.get_width() / 2, height + 10, f'{count}', 
             ha='center', va='bottom', fontsize=9)
    plt.text(bar.get_x() + bar.get_width() / 2, height - 10, f'({percentage:.1f}%)', 
             ha='center', va='top', fontsize=9)

plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()
```

```{python}
#| output: false

room_type_counts = gdf['room_type'].value_counts()
room_type_percentage = gdf['room_type'].value_counts(normalize=True) * 100

room_type_stats = pd.DataFrame({
    'Count': room_type_counts,
    'Percentage': room_type_percentage
})

room_type_stats
```

***Room type of property*** is divided into four categories.

- Entire home/apt: 63.8%
- Private room: 35.6%
- Shared room: 0.45%
- Hotel room: 0.2%

**Conclusion:**

1. The high proportion of entire homes/apt indicates that many guests prefer independent accommodations for greater privacy and autonomy.  
    This aligns with a broader shift in tourism, where more visitors are opting for alternative lodging options instead of traditional hotels to enjoy a more spacious and private environment [@zervas2017rise].
2. The 35.6% share of private rooms suggests that some guests are still willing to choose more affordable accommodations, even if it means sharing common spaces. These listings cater to budget-conscious travelers.
3. The low percentages of shared rooms and hotel rooms indicate that Airbnb's core market in London, a well-established market, tends to favor more private lodging options.

#### 6.2.2 Distribution of Minimum Nights for renting property

```{python}
#| output: false

# Create a subplot
fig, ax = plt.subplots(figsize=(8, 4))  # Single subplot

# Set values greater than 35 to 35
gdf['minimum_nights_clipped'] = np.where(gdf['minimum_nights'] > 35, 35, gdf['minimum_nights'])
bins = list(range(1, 37))  # Bins from 1 to 35, with 35+ combined into one bin

# Plot the histogram for 'minimum_nights' column
ax.hist(gdf['minimum_nights_clipped'], bins=bins, color='lightblue', edgecolor='black')

# Add a vertical line for the threshold (at 30 days)
ax.axvline(x=30, color='black', linestyle='--', linewidth=1.5, label='STR Threshold (30 days)')

# Calculate the proportion below the 30-day threshold
threshold = 30
below_threshold = gdf[gdf['minimum_nights'] < threshold].shape[0]  # Count of listings below 30 days
total_entries = gdf.shape[0]  # Total count of listings
percentage_below = (below_threshold / total_entries) * 100

# Display the percentage information on the graph
ax.text(18, ax.get_ylim()[1] * 0.8,  # Set position
        f'{below_threshold} listings ({percentage_below:.1f}%) below 30 nights',
        fontsize=12, color='black', ha='center', bbox=dict(facecolor='white', alpha=0.6))

# Set the title and labels
ax.set_title('Distribution of Minimum Nights', fontsize=15)
ax.set_xlabel('Minimum Nights')
ax.set_ylabel('Count')
ax.legend()

# Set x-axis ticks and labels
x_ticks = list(range(1, 8)) + list(range(10, 36, 5))  # Show daily ticks for the first 7 days, and every 5 days from 10 to 35
x_labels = [str(i) for i in x_ticks[:-1]] + ['35+']  # Define tick labels
ax.set_xticks(x_ticks)
ax.set_xticklabels(x_labels)

# Display the plot
plt.tight_layout()
plt.show()
```

```{python}
#| output: false

#Percentage of listings for which minimum nights are less than 7.
gdf[gdf['minimum_nights']<=7].shape[0] / total_entries # 0.9232768749285172
```

Based on the dataset, 93550 listings have a minimum night stay of less than the STR threshold (30 days), making up 97.3% of the total. Additionally, listings with a minimum stay of less than 7 days account for 92.3% of the total. 
**The London rental market on airbnb is dominated by short-term rentals.**

Chaudhary had illustrated some drawbacks of short term renting [@chaudhary2021effects].

1. **Reduced long-term housing supply**: Due to higher profits from short-term rentals (e.g., Airbnb), many landlords prioritize short-term leases over long-term rentals, exacerbating London's housing crisis and driving up rents, especially for low- and middle-income residents.
2. **Community impacts**: A high volume of short-term rentals can disrupt neighborhoods, increasing noise and tourist traffic, making communities less appealing for long-term residents and undermining stability and safety.

## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London?

### 7.1 The Correlation between Airbnb Rental Prices and Housing Prices
This part aims to explore the **spatial correlation** between **Airbnb rental prices** and **housing prices** across various wards in London. Wards are considered as the smallest unit of analysis for this research. Initially, K-means clustering is employed to categorize properties based on their rental prices. Subsequently, average Airbnb rental prices and average housing prices for each ward are calculated.

```{python}
#| output: false

import pandas as pd
import numpy as np
import geopandas as gpd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from shapely.geometry import Point
from sklearn.cluster import DBSCAN
from esda import Moran
from libpysal import weights
```

```{python}
#| output: false

# read the latest airbnb listings dataset
df22 = pd.read_csv('data/listings.csv.gz', compression='gzip', low_memory=False)
```

```{python}
#| output: false

# Create geometry column
df22['geometry'] = df22.apply(lambda x: Point(x['longitude'], x['latitude']), axis=1)

# Convert to GeoDataFrame
df22_gdf = gpd.GeoDataFrame(df22, geometry='geometry')
```

```{python}
#| output: false

# Import geographical data of London communities
london_areaer = gpd.read_file('data/London/London_Ward.shp')
```

```{python}
#| output: false

# Perform spatial join
df22_gdf = df22_gdf.set_crs(epsg=4326) 
london_areaer = london_areaer.to_crs(epsg=4326) 
```

```{python}
#| output: false

# Clean 'price' column by removing '$' and ',' and converting to float
df22_gdf['price'] = df22_gdf['price'].replace({'\$': '', ',': '', '-': None}, regex=True)

# Convert price column to float
df22_gdf['price'] = pd.to_numeric(df22_gdf['price'], errors='coerce')

# Print unique values after cleaning for checking
print(df22_gdf['price'].unique())

# Check the number of NaN values in the cleaned 'price' column
print(df22_gdf['price'].isnull().sum())

# Remove rows containing NaN
df22_gdf.dropna(subset=['price'], inplace=True)

# Re-check the number of NaN values
print(df22_gdf['price'].isnull().sum())
```

```{python}
#| output: false

# Extract price data and normalize
price_data = df22_gdf[['price']]
scaler = StandardScaler()
scaled_price = scaler.fit_transform(price_data)

# Set K-means clustering amount
kmeans = KMeans(n_clusters=5, random_state=42)

# Perform clustering
df22_gdf['cluster'] = kmeans.fit_predict(scaled_price)
```

```{python}
#| output: false

# Perform spatial join to find the community for each listing
df22_gdf = gpd.sjoin(df22_gdf, london_areaer, how='left', predicate='intersects')

# Calculate average house price for each community
rental_prices = df22_gdf.groupby('NAME')['price'].mean().reset_index(name='rental_price')
```

```{python}
#| output: false
london_areaer = london_areaer.merge(rental_prices, left_on='NAME', right_on='NAME', how='left')
```

```{python}
#| output: false

# Check merged data
print(london_areaer.head())  # Ensure 'average_price' column exists
print(london_areaer.columns)  # Check all column names
```

```{python}
#| output: false

# Read house price data
house_prices = pd.read_csv('data/land-registry-house-prices-ward.csv')
# View the first few rows of the data
print(house_prices.head())
# Replace invalid values '-' with NaN
house_prices['Value'] = house_prices['Value'].replace('-', np.nan)
# Clean 'Value' column by removing commas and converting to float
house_prices['Value'] = house_prices['Value'].str.replace(',', '').astype(float)
# Handle potential NaN values in the file
house_prices['Value'] = house_prices['Value'].fillna(0)  # Or use mean imputation or other methods
```

```{python}
#| output: false

# Import geographical data of London communities
london_areas = gpd.read_file('data/London/London_Ward.shp')
print(london_areas.head())
```

```{python}
#| output: false

# Assume house price data contains 'ward' and 'price' columns
average_prices = house_prices.groupby('Ward_name')['Value'].mean().reset_index(name='average_price')
# View the first few rows of average house price
print(average_prices.head())
```

```{python}
#| output: false

# Merge data
london_areas = london_areas.merge(average_prices, left_on='NAME', right_on='Ward_name', how='right')
print(london_areas.columns)  # Check all column names
print(london_areas.head())  # View the first few rows of merged data
```

```{python}
#| output: false

# Calculate the overall average house price
average_overall_price = house_prices['Value'].mean()

# Calculate relative prices
london_areas['relative_price'] = london_areas['average_price'] / average_overall_price

# Get the top ten relative_price values and their corresponding Ward names
top_wards = london_areas.nlargest(10, 'relative_price')[['NAME', 'relative_price']]

# Print results
print(top_wards)
```

```{python}
# Plot the map
fig, axes = plt.subplots(1, 2, figsize=(15, 10))

# Plot the Average Airbnb Rental Prices in London map
london_areaer.plot(column='rental_price', cmap='viridis',  ax=axes[0])
london_areaer.boundary.plot(ax=axes[0], color='black', linewidth=0)
axes[0].set_title("Average Airbnb Rental Prices in London")
axes[0].set_xlabel("Longitude")
axes[0].set_ylabel("Latitude")

# Retrieve color bar
cbar = plt.colorbar(axes[0].collections[0], ax=axes[0], shrink=0.3)  
cbar.set_ticks([1, 2, 3, 4, 5])  
cbar.set_ticklabels(['1', '2', '3', '4', '5']) 

# Plot the Relative Average House Prices in London ma
london_areas.plot(column='relative_price', cmap='viridis',  ax=axes[1])
london_areas.boundary.plot(ax=axes[1], color='black', linewidth=0)
axes[1].set_title("Relative Average House Prices in London")
axes[1].set_xlabel("Longitude")
axes[1].set_ylabel("Latitude")

# Retrieve color bar
cbar = plt.colorbar(axes[1].collections[0], ax=axes[1], shrink=0.3) 
cbar.set_ticks([1, 2, 3, 4, 5])  
cbar.set_ticklabels(['1', '2', '3', '4', '5']) 



plt.tight_layout()  # 自动调整子图间距
plt.show()
```

By comparing these two metrics visually on a map, it is observed that the area with **the highest Airbnb rental prices is Bishop's**, which paradoxically reflects a **relatively low average housing price**. 

Conversely, Knightsbridge and Belgravia, located in proximity to the city center, exhibit the highest average housing prices, with a **noticeable decline** as one moves outward from the central area.

Importantly, the districts that report the highest Airbnb rental prices do not coincide with those that have the highest housing prices. 

Nevertheless, **both** metrics are significantly concentrated around the ward of Knightsbridge and Belgravia. Furthermore, some suburban wards demonstrate relatively high Airbnb rental prices; however, the housing prices in these areas remain comparable to those of their neighboring regions, suggesting limited impact.

### 7.2 Multiple linear regression
In order to more intuitively prove the impact of Airbnb on the local community and explore the extent of the impact, we used the method of constructing a multiple linear regression model, where we calculated the median number of **housing price**, **population density** and **house sales** of each ward, and took them as the **dependent variables**. We calculated the median number of **Airbnb price**, **monthly number of reviews**, **annual availablity**, **review value**, and **aribnb count** as **independent variables**.

```{python}
#| echo: false

'''
explanation: '| echo: false' means not showing source code, and 
            '| output: false' means not showing output of that code chunk (print, plt.show(), etc.)
And these should be placed in the first row of each code chunk.
'''

# import library used in this qmd file
import os
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt

from requests import get
import requests
from urllib.parse import urlparse
from functools import wraps
import zipfile
import shutil
```

```{python}
#| output: false
## Data Processing 

### Airbnb data

# read the latest airbnb listings dataset
Airbnb_inital_Data = pd.read_csv('data/listings.csv.gz', compression='gzip', low_memory=False)

#expand count limitation of column display
pd.options.display.max_columns = 75 

Airbnb_inital_Data
Airbnb_inital_Data.info()

from shapely.geometry import Point

# Creat the GeoDataFrame of airbnb
Airbnb_inital_Data['geometry'] = Airbnb_inital_Data.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)

# select the columns we need
Airbnb_Data = Airbnb_inital_Data[["name","host_id","latitude","longitude","geometry","price","reviews_per_month","availability_365","review_scores_rating","review_scores_value"]]
```

```{python}
#| output: false

### LONDON WARD data

London_ward = gpd.read_file('data/London/London_Ward_CityMerged.shp')
London_ward = London_ward.to_crs("EPSG:4326")
Airbnb_gdf = gpd.GeoDataFrame(Airbnb_Data, geometry='geometry', crs=London_ward.crs)  # set the same CRS

print(Airbnb_gdf.crs)
print(London_ward.crs)
```

```{python}
#| output: false

### Clean and Join data

Airbnb_gdf = Airbnb_gdf.dropna(subset=['geometry'])
London_ward = London_ward.dropna(subset=['geometry'])
Airbnb_data_wards = gpd.sjoin(Airbnb_gdf, London_ward, how='left', predicate='within')
Airbnb_data_wards = Airbnb_data_wards.drop(columns=["LB_GSS_CD","BOROUGH","POLY_ID","HECTARES","index_right","name"])
Airbnb_data_wards['WARD'] = Airbnb_data_wards['NAME']
Airbnb_data_wards = Airbnb_data_wards.drop(columns=["NONLD_AREA","NAME"])
Airbnb_data_wards_sorted = Airbnb_data_wards.sort_values(by='WARD')#, key=lambda col: col.str[0].str.upper())

num_rows_with_nan = Airbnb_data_wards_sorted.isnull().any(axis=1).sum()
print(f"Count of columns tha contain NaN: {num_rows_with_nan}")

Airbnb_data_wards_sorted = Airbnb_data_wards_sorted.dropna()

Airbnb_final_data = Airbnb_data_wards_sorted [["WARD","GSS_CODE","price","reviews_per_month","availability_365","review_scores_rating","review_scores_value","geometry"]]

# Clean 'price' column and convert it to float
Airbnb_final_data['price'] = Airbnb_final_data['price'].astype(str)
Airbnb_final_data['price'] = Airbnb_final_data['price'].str.replace('$','', regex=False).str.replace(',','', regex=False).astype(float).fillna(-1)

Airbnb_data_wards_all = Airbnb_final_data.groupby('GSS_CODE').agg(
    
        WARD=('WARD', 'first'),  # save the first value of  ward in GSS_CODE 
        price=('price', 'median'),  # calculate the median number of  price 
        availability_365=('availability_365', 'median'),  # calculate the median number of availability_365 
        review_scores_rating=('review_scores_rating', 'median'),  # calculate the median number of review_scores_rating
        reviews_per_month=('reviews_per_month', 'median'), # calculate the median number of reviews_per_month
        review_scores_value=('review_scores_value', 'median'), # calculate the median number of review_scores_value
        count=("GSS_CODE", "size") #calculate the count
        
).reset_index()
```

```{python}
#| output: false

### Population data

# Read file
Population_density = pd.read_csv('data/housing-density-ward.csv')

Population_density = Population_density.sort_values(by = 'Code')
Population_density['Year'] = Population_density['Year'].astype(str)
PD_filter = Population_density[Population_density['Year'].str.contains('2024', na=False)]

PD_filter = PD_filter.drop(columns=["Borough","Hectares","Square_Kilometres"])

PD_filter.rename(columns={"Code": "GSS_CODE"}, inplace=True)

# Join data
Airbnb_with_population = pd.merge(Airbnb_data_wards_all, PD_filter, on="GSS_CODE", how="left")
Airbnb_with_population = Airbnb_with_population.drop(columns="Ward_Name")
Airbnb_with_population = Airbnb_with_population.dropna()
```

```{python}
#| output: false

# process the price and number of homes sold

House_price_initial = pd.read_csv('data/land-registry-house-prices-ward.csv')
House_price_initial = House_price_initial.sort_values(by = 'Code')

# Select latest data
HP_filter = House_price_initial[House_price_initial['Year'].str.contains('Year ending Dec 2017', na=False)]
HP_filter = HP_filter[HP_filter['Measure'].str.contains('Median', na=False)]
HP_filter = HP_filter.drop(columns=["Borough"])

HS_filter = House_price_initial[House_price_initial['Year'].str.contains('Year ending Dec 2017', na=False)]
HS_filter = HS_filter[HS_filter['Measure'].str.contains('Sales', na=False)]
HS_filter = HS_filter.drop(columns=["Borough"])

HS_filter = HS_filter[~HS_filter['Ward_name'].str.contains('City of London')]
HP_filter = HP_filter[~HP_filter['Ward_name'].str.contains('City of London')]

HS_filter.rename(columns={"Code": "GSS_CODE"}, inplace=True)
HP_filter.rename(columns={"Code": "GSS_CODE"}, inplace=True)

# Join data

Airbnb_with_housedata = pd.merge(Airbnb_data_wards_all, HP_filter, on="GSS_CODE", how="left")
Airbnb_with_housedata = pd.merge(Airbnb_with_housedata, HS_filter, on="GSS_CODE", how="left")
Airbnb_with_housedata = Airbnb_with_housedata.drop(columns=["Year_x","Year_y","Ward_name_x","Ward_name_y","Measure_x","Measure_y"])
Airbnb_with_housedata.rename(columns={"Value_x": "Houseprice_median"}, inplace=True)
Airbnb_with_housedata.rename(columns={"Value_y": "Housesales_median"}, inplace=True)
Airbnb_with_housedata=Airbnb_with_housedata.dropna()
Airbnb_with_housedata = Airbnb_with_housedata.drop(columns=["review_scores_rating"])
```

```{python}
#| output: false

Airbnb_with_housedata = pd.merge(Airbnb_with_housedata, Airbnb_with_population, on="GSS_CODE", how="left")
Airbnb_with_alldata = Airbnb_with_housedata.drop(columns=["WARD_y","price_y","availability_365_y","reviews_per_month_y","review_scores_value_y","count_y","Year"])
Airbnb_with_alldata = Airbnb_with_alldata.drop(columns=["review_scores_rating"])
Airbnb_with_alldata.rename(columns={"WARD_x": "WARD"}, inplace=True)
Airbnb_with_alldata.rename(columns={"price_x": "Airbnb_price"}, inplace=True)
Airbnb_with_alldata.rename(columns={"availability_365_x": "Airbnb_availability_365"}, inplace=True)
Airbnb_with_alldata.rename(columns={"reviews_per_month_x": "Reviews_per_month"}, inplace=True)
Airbnb_with_alldata.rename(columns={"review_scores_value_x": "Review_scores_value"}, inplace=True)
Airbnb_with_alldata.rename(columns={"count_x": "Airbnb_count"}, inplace=True)

# Airbnb_with_alldata.to_csv('data/Airbnb_with_all_data.csv')
```

```{python}
#| output: false

# Check the basic information of the data
print(Airbnb_with_alldata.info())
print(Airbnb_with_alldata.describe())
```

```{python}
Airbnb_with_alldata['Houseprice_median'] = Airbnb_with_alldata['Houseprice_median'].str.replace(',', '').astype(float)
Airbnb_with_alldata['Housesales_median'] = Airbnb_with_alldata['Housesales_median'].str.replace(',', '').astype(float)
```

```{python}
#| output: false

# Filtering numeric columns
numeric_columns = Airbnb_with_alldata.select_dtypes(include=['float64', 'int64']).columns

# Exclude non-numeric columns
numeric_data = Airbnb_with_alldata[numeric_columns]

# Calculating the correlation matrix
correlation_matrix = numeric_data.corr()

# The correlation between the target variable and the independent variable is extracted
target_columns = ['Population_per_square_kilometre', 'Houseprice_median', 'Housesales_median']

correlation_with_targets = correlation_matrix.loc[target_columns, ['Airbnb_price', 'Airbnb_availability_365', 'Reviews_per_month', 'Review_scores_value','Airbnb_count']]
print(correlation_with_targets)
```

```{python}

from statsmodels.stats.outliers_influence import variance_inflation_factor

# Select the required independent variables
independent_vars = Airbnb_with_alldata[['Airbnb_price', 'Airbnb_availability_365', 'Reviews_per_month', 'Review_scores_value','Airbnb_count']]
independent_vars.insert(0, 'Intercept', 1)  # Add an intercept term

# Calculate the VIF for each independent variable
vif_data = pd.DataFrame({
    "Variable": independent_vars.columns,
    "VIF": [variance_inflation_factor(independent_vars.values, i) for i in range(independent_vars.shape[1])]
})
print(vif_data)
```

After calculating the VIF of the independent variables, we find that there are no variables that exceed the threshold, so there may be no obvious multicollinearity, and the model results are as follows:

```{python}
import statsmodels.api as sm

# List of target variables
target_columns = ['Population_per_square_kilometre', 'Houseprice_median', 'Housesales_median']

# The loop builds a regression model for each target variable
for target in target_columns:
    X = Airbnb_with_alldata[['Airbnb_price', 'Airbnb_availability_365', 'Reviews_per_month', 'Review_scores_value','Airbnb_count']]
    y = Airbnb_with_alldata[target]
    X = sm.add_constant(X)  # Add an intercept term
    model = sm.OLS(y, X).fit()
    print(f"Regression results for {target}:")
    #print(model.summary())
    
    #print("R-squared:", round(model.rsquared, 3))
    #print("Adjusted R-squared:", round(model.rsquared_adj, 3))
    #print("F-statistic:", round(model.fvalue, 3))
    #print("P-values:", model.pvalues.round(3))
    #print("Coefficients:", model.params.round(3))
    
    # save result as picture
    #fig, ax = plt.subplots(figsize=(10, 8))
    #ax.text(0, 1, model.summary(), fontsize=10, family='monospace', verticalalignment='top')
    #ax.axis('off')  # 关闭坐标轴
    #plt.savefig('regression_results.png', bbox_inches='tight', dpi=300)
    #plt.show()

    # select the main parameters
    results_df = pd.DataFrame({
    'Coefficient': model.params.round(3),
    'Standard Error': model.bse.round(3),
    't-value': model.tvalues.round(3),
    'P-value': model.pvalues.round(3),
    'R-squared': round(model.rsquared, 3),
     #'Adj R-squared': round(model.rsquared_adj, 3),
    #'F-statistic': round(model.fvalue, 3)
})
    #print(tabulate(results_df, headers='keys', tablefmt='grid'))
    display(results_df)
```

The model fits well, including:

- The Houseprice_median model performed best, explaining 53.2% of the fluctuations
- Population_per_square_kilometre model was second, explaining 32.4% of the fluctuations.
- The Housesales_median model performs the worst, explaining only 7.8%.

- Airbnb_count is significant in all three models and the effect is positive.
- Reviews_per_month and Review_scores_value are significant in some models, but in different directions.
- Airbnb_price is only significant in the Houseprice_median model.

```{python}
#| output: false

import seaborn as sns
import matplotlib.pyplot as plt

# Distribution of variables
sns.pairplot(numeric_data[['Houseprice_median', 'Airbnb_price', 'Airbnb_availability_365', 'Reviews_per_month', 'Review_scores_value','Airbnb_count']])
plt.show()

# Regression residual analysis
sns.residplot(x=model.predict(X), y=model.resid)
plt.xlabel('Fitted Values')
plt.ylabel('Residuals')
plt.show()
```

### 7.3 The Impact of Airbnb on London Neighborhoods: K-Means Geodemographic Classification

#### 7.3.1 Data processing

 *key variables:*    
 - average_price: Average nightly price of Airbnb  
 - airbnb_density  
 - Price_Change: Five-year housing price change in the community  
 - People per Sq Km: Population density  
 - Income Score (rate): Income deprivation score  
 - hotel_density  
 -  tourist_attraction_density  
 -  AvPTAI2015: Public Transport Accessibility Index   

*Variables that need to be processed :*

 - Calculate the density of tourist attractions in each LSOA in London

```{python}
#| output: false
!pip install --upgrade osmnx
import osmnx as ox
print(ox.__version__)  # Should be 1.2.2 or newer
```

```{python}
#| output: false
import geopandas as gpd
import pandas as pd

#  Load London LSOA boundary data
lsoa_boundaries = gpd.read_file("data/London/LSOA_2011_London_gen_MHW.shp")

# Retrieve tourist attraction data for London
# Use OSMnx to extract nodes tagged as `tourism=attraction`
tourist_attractions = ox.features_from_place(
    "London, England", tags={"tourism": "attraction"}
)

# Ensure the coordinate reference system (CRS) is consistent
tourist_attractions = tourist_attractions.to_crs(lsoa_boundaries.crs)

# : Count the number of tourist attractions within each LSOA
lsoa_boundaries["tourist_attraction_count"] = lsoa_boundaries.apply(
    lambda row: tourist_attractions.within(row.geometry).sum(), axis=1
)

#  Calculate the density of tourist attractions (per square kilometer)
# Transform to an equal-area projection to compute accurate area
lsoa_boundaries = lsoa_boundaries.to_crs({"proj": "cea"})
lsoa_boundaries["area_km2"] = lsoa_boundaries.geometry.area / 1e6  # Convert to square kilometers

# Tourist attraction density = count / area
lsoa_boundaries["tourist_attraction_density"] = (
    lsoa_boundaries["tourist_attraction_count"] / lsoa_boundaries["area_km2"]
)

#  Export results to a CSV file
# Use 'LSOA11CD' as the LSOA code column
output_path = "data/london_lsoa_tourist_density.csv"
output_csv = lsoa_boundaries[["LSOA11CD", "tourist_attraction_count", "tourist_attraction_density"]]
output_csv.to_csv(output_path, index=False)

print(f"Tourist attraction density has been saved to {output_path}")
```

 - Calculate hotel density for each LSOA unit

```{python}
#| output: false
from shapely.geometry import Point

# 1. Load LSOA boundary data
lsoa_shp_path = "data/London/LSOA_2011_London_gen_MHW.shp"
lsoa_data = gpd.read_file(lsoa_shp_path)

# (EPSG:27700 - British National Grid)
lsoa_data = lsoa_data.to_crs("EPSG:27700")
lsoa_data['area_km2'] = lsoa_data['geometry'].area / 10**6

lsoa_data = lsoa_data.to_crs("EPSG:4326")

# 2. Get the London boundary 
london_boundary = lsoa_data.dissolve().geometry.iloc[0]

# 3. Fetch hotel data from OpenStreetMap
hotels = ox.features_from_polygon(london_boundary, tags={"tourism": "hotel"}) 

# Convert the hotel data into a GeoDataFrame and retain only geometry
hotels_gdf = gpd.GeoDataFrame(hotels[['geometry', 'name']].dropna(subset=['geometry']), crs="EPSG:4326")

# 4. Spatially join hotels to their respective LSOA units
hotels_in_lsoa = gpd.sjoin(hotels_gdf, lsoa_data, how="inner", predicate="within")

# Count the number of hotels per LSOA
hotel_counts = hotels_in_lsoa.groupby('LSOA11CD').size().reset_index(name='hotel_count')

# 5. Calculate hotel density (number of hotels per square kilometer)
lsoa_data = lsoa_data.merge(hotel_counts, on='LSOA11CD', how='left')
lsoa_data['hotel_count'] = lsoa_data['hotel_count'].fillna(0)
lsoa_data['hotel_density'] = lsoa_data['hotel_count'] / lsoa_data['area_km2']

# 6. Save results to a CSV file
output_csv_path = "data/lsoa_hotel_density.csv"
lsoa_data[['LSOA11CD', 'hotel_count', 'area_km2', 'hotel_density']].to_csv(output_csv_path, index=False)

print(f"Hotel density data saved to {output_csv_path}")
```

 - Calculate five-year house price changes for each LSOA unit

```{python}
#| output: false
file_path_xlsx = "data/LSOA_Median_properties_price.xlsx"

# Load the data
data_xlsx = pd.read_excel(file_path_xlsx, engine='openpyxl')

# List of all London boroughs including the City of London
london_boroughs = [
    "City of London", "Barking and Dagenham", "Barnet", "Bexley", "Brent", "Bromley", "Camden",
    "Croydon", "Ealing", "Enfield", "Greenwich", "Hackney", "Hammersmith and Fulham", "Haringey",
    "Harrow", "Havering", "Hillingdon", "Hounslow", "Islington", "Kensington and Chelsea",
    "Kingston upon Thames", "Lambeth", "Lewisham", "Merton", "Newham", "Redbridge",
    "Richmond upon Thames", "Southwark", "Sutton", "Tower Hamlets", "Waltham Forest", "Wandsworth", "Westminster"
]

# Filter the data for London based on the local authority name
london_data = data_xlsx[data_xlsx['Local authority name'].isin(london_boroughs)]

# Save the filtered data to a CSV file
output_csv_path = "data/London_Housing_Data.csv"
london_data.to_csv(output_csv_path, index=False)

print(f"Filtered London data has been saved to {output_csv_path}")
```

```{python}
#| output: false

# Load the data
data = pd.read_csv('data/London_Housing_Data.csv')

# Strip any leading/trailing spaces in the column names
data.columns = data.columns.str.strip()

# Convert 'Year ending Mar 2023' and 'Year ending Dec 2018' to numeric (if they are not already)
data['Year ending Mar 2023'] = pd.to_numeric(data['Year ending Mar 2023'], errors='coerce')
data['Year ending Dec 2018'] = pd.to_numeric(data['Year ending Dec 2018'], errors='coerce')

# Check for missing values in the year columns after conversion
missing_values = data[['Year ending Mar 2023', 'Year ending Dec 2018']].isnull().sum()
print(f"Missing values in Year columns:\n{missing_values}")

# Create a copy of the data to avoid SettingWithCopyWarning
data_clean = data.copy()

# Drop rows with missing values for price calculations (only for the year columns)
data_clean = data_clean.dropna(subset=['Year ending Mar 2023', 'Year ending Dec 2018'])

# Calculate the price change rate between 'Year ending Mar 2023' and 'Year ending Dec 2018'
# Use .loc to avoid SettingWithCopyWarning
data_clean.loc[:, 'Price_Change'] = ((data_clean['Year ending Mar 2023'] - data_clean['Year ending Dec 2018']) / data_clean['Year ending Dec 2018'] * 100)

# Merge the price change back with the original data (using LSOA_code as the key)
data_with_change = pd.merge(data, data_clean[['LSOA code', 'Price_Change']], on='LSOA code', how='left')

# Ensure data is sorted by LSOA code
data_with_change_sorted = data_with_change.sort_values(by=['LSOA code'])

# Save the result to a new CSV file
data_with_change_sorted[['LSOA code', 'Price_Change']].to_csv('data/LSOA_London_houseprice_change.csv', index=False)

print("House price change data saved to 'LSOA_London_houseprice_change.csv'")
```

```{python}
#| output: false
# Define paths for the input Excel files and output CSV files
input_file_imd = "data/LSOA_IMD_london.xlsx"
output_file_imd = "data/LSOA_IMD_london.csv"

input_file_population = "data/LSOA_population_density.xlsx"
output_file_population = "data/LSOA_population_density.csv"

# Function to convert an Excel file to a CSV file
def convert_excel_to_csv(input_path, output_path):
    # Read the Excel file
    data = pd.read_excel(input_path)
    # Save the data to a CSV file
    data.to_csv(output_path, index=False)
    print(f"Converted {input_path} to {output_path}")

# Convert LSOA_IMD_london.xlsx to LSOA_IMD_london.csv
convert_excel_to_csv(input_file_imd, output_file_imd)

# Convert LSOA_population_density.xlsx to LSOA_population_density.csv
convert_excel_to_csv(input_file_population, output_file_population)
```

- Read in and process Airbnb data

```{python}
#| output: false
# read in Airbnb data
airbnb_data = pd.read_csv('data/listings.csv.gz', compression='gzip', low_memory=False)
```

```{python}
#| output: false
import geopandas as gpd
import pandas as pd
from shapely.geometry import Point

# Filter out only the columns we need
columns_to_keep = ['id', 'property_type', 'room_type', 'price', 'review_scores_rating', 'review_scores_value', 'longitude', 'latitude']
airbnb_filtered = airbnb_data[columns_to_keep].copy() 

airbnb_filtered.loc[:, 'geometry'] = airbnb_filtered.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)

# Create a GeoDataFrame
airbnb_gdf = gpd.GeoDataFrame(airbnb_filtered, geometry='geometry', crs="EPSG:4326")  # Ensure CRS is WGS84

airbnb_filtered.to_csv('data/filtered_airbnb_data.csv', index=False)

airbnb_gdf.to_file('data/filtered_airbnb_data.geojson', driver='GeoJSON')

# Print the cleaned data
print(airbnb_gdf.head())
```

 - Calculate Airbnb Density and Average Price per Night for LSOA Units

```{python}
#| output: false
from shapely.geometry import Point

# Load the LSOA boundary data
lsoa_shp_path = 'data/London/LSOA_2011_London_gen_MHW.shp'
lsoa_data = gpd.read_file(lsoa_shp_path)

# Use the projected coordinate system to calculate the area (EPSG:27700 - British National Grid)
lsoa_data = lsoa_data.to_crs("EPSG:27700")
lsoa_data['area_km2'] = lsoa_data.geometry.area / 10**6  # Convert area from square meters to square kilometers

# Reproject back to WGS84 for subsequent spatial operations
lsoa_data = lsoa_data.to_crs("EPSG:4326")

# Load the Airbnb data with latitude and longitude
airbnb_df = pd.read_csv('data/filtered_airbnb_data.csv')

# Ensure the Airbnb data has latitude and longitude columns and create geometries
airbnb_df['geometry'] = airbnb_df.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)

# Convert DataFrame to GeoDataFrame
airbnb_gdf = gpd.GeoDataFrame(airbnb_df, geometry='geometry', crs="EPSG:4326")

# Spatial join to get the Airbnb points within each LSOA
airbnb_in_lsoa = gpd.sjoin(airbnb_gdf, lsoa_data, how="inner", predicate="within")

# Clean the price column
airbnb_in_lsoa['price'] = airbnb_in_lsoa['price'].replace({'\$': '', ',': ''}, regex=True).astype(float)

# Calculate the average price for each LSOA
average_price = airbnb_in_lsoa.groupby('LSOA11CD')['price'].mean().reset_index()
average_price.rename(columns={'price': 'average_price'}, inplace=True)

# Calculate the number of Airbnb listings in each LSOA
airbnb_density = airbnb_in_lsoa.groupby('LSOA11CD')['price'].count().reset_index(name='airbnb_count')

# Merge average price and Airbnb count with LSOA data
lsoa_data = lsoa_data.merge(average_price, on='LSOA11CD', how='left')
lsoa_data = lsoa_data.merge(airbnb_density, on='LSOA11CD', how='left')

# Calculate Airbnb density (number of listings per square kilometer)
lsoa_data['airbnb_density'] = lsoa_data['airbnb_count'] / lsoa_data['area_km2']

# Display the result
print(lsoa_data[['LSOA11CD', 'average_price', 'airbnb_count', 'airbnb_density']].head())
```

 - Merge all data

```{python}
#| output: false
# Load all the datasets
airbnb_df = pd.read_csv('data/LSOA_airbnb_summary.csv')
houseprice_df = pd.read_csv('data/LSOA_London_houseprice_change.csv')
pop_density_df = pd.read_csv('data/LSOA_population_density.csv')
imd_df = pd.read_csv('data/LSOA_IMD_london.csv')
hotel_density_df = pd.read_csv('data/lsoa_hotel_density.csv')
tourist_density_df = pd.read_csv('data/london_lsoa_tourist_density.csv')
avptai_df = pd.read_csv('data/LSOA_AvPTAI.csv')

#  Merge the datasets based on the correct columns
merged_data = airbnb_df.merge(houseprice_df, left_on='LSOA11CD', right_on='LSOA code', how='left')
merged_data = merged_data.merge(pop_density_df, left_on='LSOA11CD', right_on='LSOA Code', how='left')
merged_data = merged_data.merge(imd_df, left_on='LSOA11CD', right_on='LSOA code (2011)', how='left')
merged_data = merged_data.merge(hotel_density_df, on='LSOA11CD', how='left')
merged_data = merged_data.merge(tourist_density_df, on='LSOA11CD', how='left')
merged_data = merged_data.merge(avptai_df, left_on='LSOA11CD', right_on='LSOA2011', how='left')

# Select only the relevant columns
final_data = merged_data[['LSOA11CD', 'average_price', 'airbnb_density', 'Price_Change', 
                          'People per Sq Km', 'Income Score (rate)', 'hotel_density', 
                          'tourist_attraction_density', 'AvPTAI2015']]

# Check the first few rows of the final data
print(final_data.head())

# Optionally, save the final merged data to a CSV file
final_data.to_csv('data/final_LSOA_data.csv', index=False)
```

 - Check and clean the data  

```{python}
#| output: false
# Step 1: Check the column names for consistency
print(final_data.columns)

# Step 2: Strip any leading/trailing spaces from column names
final_data.columns = final_data.columns.str.strip()

# Step 3: Check for missing values
print(final_data.isnull().sum())

# Step 4: Fill missing values for numerical columns using the median, using `.loc` to avoid SettingWithCopyWarning
numerical_columns = ['average_price', 'airbnb_density', 'Price_Change', 'People per Sq Km', 
                     'Income Score (rate)', 'hotel_density', 'tourist_attraction_density', 'AvPTAI2015']

for col in numerical_columns:
    if col in final_data.columns:
        final_data.loc[:, col] = final_data[col].fillna(final_data[col].median())

# Step 5: Check if any missing values remain
print(final_data.isnull().sum())
```

#### 7.3.2 Standardization (Z-score scaling)

```{python}
#| output: false
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Print the column names
print(final_data.columns)

# Dynamically create numerical_columns
numerical_columns = [col for col in ['average_price', 'airbnb_density', 'price_change', 
                     'people per sq km', 'income score (rate)', 
                     'hotel_density', 'tourist_attraction_density', 'avptai2015'] if col in final_data.columns]

print("Columns to be standardized:", numerical_columns)

# Initialize StandardScaler
scaler = StandardScaler()

# Safely apply standardization using .loc
final_data.loc[:, numerical_columns] = scaler.fit_transform(final_data[numerical_columns])

# Check the result
print(final_data.head())
```

#### 7.3.3 Elbow Method to caculate the k value

```{python}
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import numpy as np
import pandas as pd

# Set random seeds to ensure reproducibility
random_state = 42
np.random.seed(random_state)

# Create a copy of the dataframe (ensure the data is stable and sorted)
final_data = final_data.copy()
final_data = final_data.sort_values(by=final_data.columns.tolist()).reset_index(drop=True)

# Select relevant columns (excluding 'LSOA11CD' for clustering)
clustering_data = final_data.drop(columns=['LSOA11CD'])

# Standardize the data (Z-score scaling)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(clustering_data)

# Elbow Method for Optimal K
plt.figure(figsize=(8, 5))
inertia = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)  # Fix n_init
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

# Plot Elbow Method
plt.plot(K_range, inertia, marker='o')
plt.title("Elbow Method for Optimal K")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Inertia")
plt.grid(True)
plt.show()
```

 - Based on the information provided in the graphs, the optimal number of clusters (K) appears to be 4.

#### 7.3.4 K-means Clustering

```{python}
#| output: false
import numpy as np
from sklearn.cluster import KMeans

# Set the random state for reproducibility
random_state = 42
np.random.seed(random_state)

# Assume final_data is your dataframe and X_scaled is your scaled feature data
# Perform K-Means clustering with fixed random state, n_init, and initial centers

optimal_k = 4  # Optimal number of clusters

# Fixing the initial centroids using k-means++ initialization
# k-means++ is deterministic when the random state is set
kmeans = KMeans(n_clusters=optimal_k, init='k-means++', n_init=10, random_state=random_state)

# Fit the model and predict clusters
final_data['Cluster'] = kmeans.fit_predict(X_scaled)

# Display clustering results
print("Clustering results:")
print(final_data[['LSOA11CD', 'Cluster']].head())

# Analyze the mean characteristics of each cluster (numerical features only)
numerical_columns = [col for col in final_data.columns if col != 'LSOA11CD' and col != 'Cluster']
cluster_summary = final_data.groupby('Cluster')[numerical_columns].mean()
print("\nCluster summary (mean values of features):")
print(cluster_summary)

# Count the number of neighborhoods in each cluster
cluster_counts = final_data['Cluster'].value_counts()
print("\nNumber of neighborhoods in each cluster:")
print(cluster_counts)

# Save the clustering results to a CSV file
final_data.to_csv('data/london_airbnb_clustering_results.csv', index=False)
print("\nClustering results have been saved to 'data/london_airbnb_clustering_results.csv'.")
```

**Results of K-means clustering**

1. Cluster 0: "Low-Impact Peripheral Areas" ：Low Airbnb activity, poor transport access, rising housing prices, minimal tourism. 

2. Cluster 1: "Heavily Impacted Tourist Hubs": Extremely high Airbnb density, central locations, high hotel and tourist attraction density, excellent transport access.

3. Cluster 2: "At-Risk Transition Zones": Moderate Airbnb activity, rising housing prices, income deprivation, low tourism and hotel presence.

4. Cluster 3: "Emerging Impact Zones": Growing Airbnb density, rising housing prices, near-central areas, moderate tourism and transport access.

#### 7.3.5 Plot K-Means Clustering Result

```{python}
import matplotlib.pyplot as plt
import geopandas as gpd
import pandas as pd
from matplotlib.patches import Patch
from matplotlib.legend_handler import HandlerPatch
from IPython.display import Image  # 用于显示保存的图片

# Load LSOA shapefile
shapefile_path = 'data/London/LSOA_2011_London_gen_MHW.shp'
lsoa_gdf = gpd.read_file(shapefile_path)

# Load clustering results
clustering_results = pd.read_csv('data/london_airbnb_clustering_results.csv')
clustering_results.rename(columns={'lsoa11cd': 'LSOA11CD'}, inplace=True)
lsoa_gdf = lsoa_gdf.merge(clustering_results[['LSOA11CD', 'Cluster']], on='LSOA11CD', how='left')

# Ensure the CRS matches
lsoa_gdf = lsoa_gdf.to_crs("EPSG:4326")

# Create a map plot
fig, ax = plt.subplots(figsize=(12, 12))

# Define softer and more pastel cluster colors, with cluster 0 as a transparent light blue
cluster_colors = {
    0: (0.6, 0.8, 1.0, 0.2),  # Transparent light blue (R, G, B, Alpha)
    1: '#FCD34D',  # Soft peach
    2: '#93C5FD',  # Soft blue
    3: '#0284C7'   # Vibrant blue (to make cluster 3 more prominent)
}

# Custom legend handler for PatchCollection
class HandlerColoredPatch(HandlerPatch):
    def create_artists(self, legend, orig_handle,
                       xdescent, ydescent, width, height, fontsize, trans):
        return [Patch(facecolor=orig_handle.get_facecolor(), edgecolor='gray', linewidth=0.3)]

# Plot LSOA regions colored by cluster
for cluster, color in cluster_colors.items():
    if isinstance(color, tuple):  # If the color is in RGBA format (transparent)
        lsoa_gdf[lsoa_gdf['Cluster'] == cluster].plot(
            ax=ax, color=color, edgecolor='gray', linewidth=0.3, label=f'Cluster {cluster}'
        )
    else:  # For solid colors
        lsoa_gdf[lsoa_gdf['Cluster'] == cluster].plot(
            ax=ax, color=color, edgecolor='gray', linewidth=0.3, label=f'Cluster {cluster}'
        )

# Layout and legend styling
ax.set_title("K-Means Clustering Results by LSOA", fontsize=16, fontweight='light')
ax.set_xlabel("Longitude", fontsize=12)
ax.set_ylabel("Latitude", fontsize=12)

# Adjust the legend to handle custom colors
legend_labels = [Patch(facecolor=color, edgecolor='gray', label=f'Cluster {cluster}') for cluster, color in cluster_colors.items()]
ax.legend(handles=legend_labels, title="Clusters", loc='upper right', fontsize=12)

# Remove axis for a cleaner map
ax.set_axis_off()

# Save the map to a file
output_image_path = 'data/london_airbnb_clusters_map.png'
plt.tight_layout()
plt.savefig(output_image_path)
plt.show()
```

- *From the map:*
- Cluster 1 - Concentrated in the central core of London, these are the high-impact tourist hubs. Such as, Southwark, Tower Hamlets and Camden.
- Cluster 2 - Located between Cluster 1 and Cluster 0, these are the transitional at-risk zones.
- Cluster 3 - Situated near the center, like Westminster, Kensington. These are the emerging impact zones on the periphery of the core.
- Cluster 0 - Found in the outer peripheral areas, these are the low-impact neighborhoods.

#### 7.3.6 Conclusion on Airbnb Impact and Policy Recommendations:

1. Heavily Impacted Central Tourist Hubs:
  - Airbnb Impact: Severe housing pressures, tourism-driven displacement, and rising rents. Airbnb dominates short-term rentals, reducing long-term housing availability.
  - Policy Recommendations: Immediate regulation to control Airbnb density. Implement rental caps and enforce zoning restrictions. Protect affordable housing for local residents.

2. At-Risk Transition Zones:
  - Airbnb Impact:Emerging Airbnb activity increases risks of gentrification, especially in deprived areas. Communities face housing affordability issues as prices rise.
  - Policy Recommendations: Introduce early interventions to stabilize housing costs. Encourage long-term rentals over short-term stays.

3. Emerging Impact Zones:
 - Airbnb Impact: Rising Airbnb density puts growing pressure on housing availability and affordability. Areas are on the path to becoming heavily impacted.
 - Policy Recommendations: Balance tourism opportunities with protecting housing for locals.

4. Emerging Impact — High Price Pressure Areas:
- Airbnb Impact: Airbnb activity is very low, with little effect on housing or rents. Poor public transport makes these areas less attractive to visitors.
- Policy Recommendations: Focus on improving transport and infrastructure to support balanced development.

### 8. Conclusion and Reflection

This study explores Airbnb's impact on London's neighbourhoods.The analysis indicates that high Airbnb rental prices do not always correspond with high housing prices, with areas like Bishop's showing higher Airbnb rents despite lower housing costs. The multiple linear regression model indicate that community housing prices are most affected by Airbnb, and population and house sales are also affected to varying degrees. K-means clustering categorizes areas into tourist hubs, at-risk zones, emerging areas, and low-impact zones. The study recommends regulating Airbnb density in high-impact areas, protecting affordable housing, and balancing tourism with long-term housing needs. Moreover, challenges related to accuracy, legality, and ethics must be acknowledged. Future research should combine more comprehensive data to enhance the result.

## Sustainable Authorship Tools

Using the Terminal in Docker, you compile the Quarto report using `quarto render <group_submission_file>.qmd`.

Your QMD file should automatically download your BibTeX and CLS files and any other required files. If this is done right after library loading then the entire report should output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References

