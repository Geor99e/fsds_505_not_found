---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Group Name's Group Project
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python (base)
    language: python
    name: base
---

```{python}
#| echo: false

'''
explanation: '| echo: false' means not showing source code, and 
            '| output: false' means not showing output of that code chunk (print, plt.show(), etc.)
And these should be placed in the first row of each code chunk.
'''

# import library used in this qmd file
import os
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt

from requests import get
import requests
from urllib.parse import urlparse
from functools import wraps
import zipfile
import shutil
```

```{python}
#| output: false

# Download the whole data file from remote repository:

# configuration parameter
GITHUB_ZIP_URL = "https://github.com/YYY677/fsds_505_not_found/archive/refs/heads/master.zip"
TARGET_FOLDER = "fsds_505_not_found-master/data"  # GitHub path
LOCAL_FOLDER = "./data"  # local file path

try:
    # check if the folder already exists
    if os.path.exists(LOCAL_FOLDER):
        print(f"The folder '{LOCAL_FOLDER}' already exists locally.")
    else:
        # download zip file from GitHub
        zip_path = "repo.zip"
        print("Downloading the repository ZIP file...")
        try:
            response = requests.get(GITHUB_ZIP_URL, timeout=30)
            response.raise_for_status()  # Raise an exception for HTTP errors
        except requests.exceptions.RequestException as e:
            print(f"Error downloading the ZIP file: {e}")
            exit(1)

        # write the ZIP file locally
        try:
            with open(zip_path, "wb") as f:
                f.write(response.content)
        except IOError as e:
            print(f"Error saving the ZIP file: {e}")
            exit(1)

        # extract the ZIP file
        print("Extracting the ZIP file...")
        try:
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(".")
        except zipfile.BadZipFile as e:
            print(f"Error extracting the ZIP file: {e}")
            os.remove(zip_path)
            exit(1)

        # move the target folder to the desired location
        print("Moving the target folder to the desired location...")
        try:
            shutil.move(TARGET_FOLDER, LOCAL_FOLDER)
        except FileNotFoundError as e:
            print(f"Error moving the folder: {e}")
            exit(1)
        except shutil.Error as e:
            print(f"Error during folder move: {e}")
            exit(1)

        # clean up temporary files
        print("Cleaning up temporary files...")
        try:
            os.remove(zip_path)
            shutil.rmtree("fsds_505_not_found-master")
        except Exception as e:
            print(f"Error during cleanup: {e}")
            # Not exiting here as the main task is complete

        print(f"The folder has been successfully downloaded to '{LOCAL_FOLDER}'.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")
```

```{python}
#| output: false

# This chunk is to down seperate files from remote repo:

def download_file(url, save_path):
    """
    Downloads a file from the given URL and saves it to the specified local path.
    """
    response = requests.get(url)
    if response.status_code == 200:
        with open(save_path, 'wb') as file:
            file.write(response.content)
        print(f"Download completed: {save_path}")
    else:
        print(f"Download failed with HTTP status code {response.status_code}, URL: {url}")

def check_and_download(url, local_path):
    """
    Checks if a file exists locally; if not, downloads it from the given URL.
    """
    if not os.path.exists(local_path):
        print(f"File does not exist. Downloading: {local_path}")
        download_file(url, local_path)
    else:
        print(f"File already exists: {local_path}")

# URLs and corresponding local save paths
url_csl = 'https://raw.githubusercontent.com/YYY677/fsds_505_not_found/master/harvard-cite-them-right.csl'
url_bib = 'https://raw.githubusercontent.com/YYY677/fsds_505_not_found/master/bio.bib'

# Specify local file paths
local_csl = "harvard-cite-them-right.csl"
local_bib = "bio.bib"

# Check and download the files
check_and_download(url_csl, local_csl)
check_and_download(url_bib, local_bib)
```

## Declaration of Authorship {.unnumbered .unlisted}

We, `505 not found`, pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date:

Student Numbers: 

## Brief Group Reflection

| What Went Well | What Was Challenging |
| -------------- | -------------------- |
| A              | B                    |
| C              | D                    |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

{{< pagebreak >}}

# Response to Questions

See the raw file for examples of how to hide computational output as there is code hidden here.

## 1. Who collected the InsideAirbnb data?

::: {.duedate}

Inside Airbnb data was collected by Murray Cox, a data activist and the project's founder, John Morris, the website designer and report producer, and Taylor Higgins, a master's student focusing on sustainable tourism at the Università degli Studi di Firenze.

:::

## 2. Why did they collect the InsideAirbnb data?

::: {.duedate}

The purpose of InsideAirbnb data is to provide data-driven insights into the impact of Airbnb on residential housing markets, thereby contributing to public discourse on the regulation and effects of short-term rental platforms in urban areas.

:::

## 3. How did they collect it?

::: {.duedate}

The data is collected by utilizing web scraping techniques such as self-made bots, inside Airbnb and AirDNA [@Pawlicz2021UNDERSTANDINGSR] [@Prentice2023AddressingDQ] to extract publicly available information from Airbnb’s website, focusing on various aspects of listings such as location, price, availability, and host details. This approach allows for the assembly of comprehensive datasets, which are then cleansed and organized to facilitate thorough analysis.

:::


## 4. How does the method of collection (Q3) impact the completeness and/or accuracy of the InsideAirbnb data? How well does it represent the process it seeks to study, and what wider issues does this raise?

::: {.duedate}

The data collection method used by InsideAirbnb raises data quality issues, mainly related to data incompleteness, reliance on website structure, and technical challenges. In terms of accuracy, data is automatically retrieved from the website, which possess the risk of capturing inaccurate or outdated information due to the dynamic nature of web content [@krotov2023big].

In addition, due to privacy measures, the geographic coordinates provided by Airbnb may not reflect the exact location of the listing, which adds a layer of inaccuracy. And as web scraping depends heavily on the structure of the Airbnb website. Changes to the website layout or measures to block scraping activities may disrupt data collection efforts, like Airbnb's anti-scrap measures including CAPTCHA or IP bans, pose additional challenges [@Prentice2023AddressingDQ]. This burdens data analysts by requiring them to constantly develop and maintain scraping scripts. 

In the discussion of the structure of InsideAirbnb data, it contains all aspects of the Airbnb market, including the distribution and characteristics of listings, pricing models, and the impact of Airbnb on the local housing market. And it is a relatively complete dataset and can assist with comprehensive analysis study.

Besides the accuracy concerns, the use of InsideAirbnb data raises technical, legal, and ethical issues.
Legally, as discussed in Sobel [@sobel2021new], scraping faces challenges in different jurisdictions, depending on how it intersects with privacy laws and terms of service agreements. This could affect the legality of the Inside Airbnb data collection process, especially if it violates Airbnb’s terms of service. Scraping also raises ethical issues, particularly regarding the consent of data subjects (Airbnb hosts and guests) whose information is collected without explicit permission. This raises significant privacy issues, as highlighted in the study by Xie and Karan [@xie2019consumers], where users’ awareness and concerns about how their data is used influence their privacy management behaviours.

:::

## 5. What ethical considerations does the use of the InsideAirbnb data raise? 

::: {.duedate}

The use of the InsideAirbnb database does raise several ethical considerations.

Firstly, there are issues of legal compliance. Web scraping can conflict with legal standards and ethical norms, particularly when data is collected without explicit consent, potentially leading to legal actions [@krotov2023big].

Secondly, privacy concerns for individuals must be addressed. Although the data might be publicly accessible, individuals typically do not anticipate their rental information being extensively aggregated and analyzed [@brenning2023web].

In many instances, data subjects (hosts and guests) are neither directly informed nor asked for consent when their data is scraped and analyzed. This presents a significant ethical dilemma: using their information without explicit permission, especially when such data might be utilized to draw conclusions or influence policies that could directly impact them.

Moreover, there is the issue of how policymaking might be influenced by the data. Since the scraped data can contain errors, issues with accuracy and potential misrepresentation may lead to misleading conclusions that could negatively affect Airbnb hosts, guests, and policy decisions.

Additionally, the misuse of data poses a significant ethical concern. When analyzing Inside Airbnb data, it is crucial to ensure that the data is not used for purposes unintended by the original data providers, such as market manipulation, unfair competition, or research that adversely impacts hosts and guests.

Lastly, transparency and accountability are crucial. Ethical research involving data scraping should clearly disclose its methodologies, the specific data collected, and how this data is utilized. Such transparency is especially important for accountability, particularly if the research has the potential to influence public opinion or policy [@brenning2023web].

:::



## 6. With reference to the InsideAirbnb data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and the types of properties that they list suggest about the nature of Airbnb lettings in London? 

### 6.1 Analysis of Hosts

```{python}
# read the latest airbnb listings dataset
df22 = pd.read_csv('data/listings.csv.gz', compression='gzip', low_memory=False)
```

```{python}
#| output: false

#expand count limitation of column display
pd.options.display.max_columns = 75 

# df22.info()
```

```{python}
# convert df(pandas) file to gdf(geopandas, which contains spatial column)
gdf = gpd.GeoDataFrame(df22, crs='EPSG:4326', geometry=gpd.points_from_xy(df22.longitude, df22.latitude)).to_crs(27700)

gdf = gdf[['host_since', 'host_id', 'host_listings_count', 'property_type', 'room_type', 'price', 'minimum_nights']]
# gdf.info()
```

```{python}
#| output: false

# This code chunk is about Wrangling Data

# Ensure 'host_since' is in datetime format
gdf['host_since'] = pd.to_datetime(gdf['host_since'], errors='coerce')
# Drop rows with NaT in 'host_since'
gdf = gdf.dropna(subset=['host_since'])

# Extract the year from 'host_since'
gdf['year'] = gdf['host_since'].dt.year.fillna(-1).astype(int)

# Handle missing values in 'host_listings_count'
gdf['host_listings_count'] = gdf['host_listings_count'].fillna(-1).astype(int)

# Clean 'price' column and convert it to float
gdf['price'] = gdf['price'].astype(str)
gdf['price'] = gdf['price'].str.replace('$','', regex=False).str.replace(',','', regex=False).astype(float).fillna(-1)

# Convert 'property_type' and 'room_type' to categorical data types
gdf['property_type'] = gdf['property_type'].astype('category')
gdf['room_type'] = gdf['room_type'].astype('category')

# gdf.head()
```

```{python}
#| output: false

gdf.info()
gdf['year'].min()
```

#### 6.1.1 Distribution of the Number of Listings per Host

```{python}
#| output: false

# This code chunk is to look at 'Distribution of the Number of Listings per Host'.

# Group by 'host_id' to count the number of listings per host
host_counts = gdf.groupby('host_id').size()

# Define bins and labels for the grouping
bins = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, float('inf')]  # 分组区间
labels = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10+']  # 分组标签

# Group the host counts into bins
host_counts_bins = pd.cut(host_counts, bins=bins, labels=labels, right=True)

# Count total listings per group (number * number of listings per host)
listings_per_group = host_counts.groupby(host_counts_bins).apply(lambda x: (x * x.index.to_series().value_counts()).sum())

# Calculate total number of listings
total_listings = listings_per_group.sum()
percentages = (listings_per_group / total_listings) * 100

# Plotting the bar chart
plt.figure(figsize=(8, 6))
bars = plt.bar(listings_per_group.index, listings_per_group.values, color='skyblue')

# Add labels with counts and percentages
for bar, count, pct in zip(bars, listings_per_group.values, percentages):
    plt.text(
        bar.get_x() + bar.get_width() / 2,  # x 坐标
        bar.get_height() + 0.5,  # y 坐标
        f'{count:.0f}\n({pct:.1f}%)',  # 标签内容
        ha='center', va='bottom', fontsize=10
    )

# Set the title and axis labels
plt.title('Distribution of Listings by Listings per Host', fontsize=16)
plt.xlabel('Listings per Host (Grouped)', fontsize=14)
plt.ylabel('Total Listings', fontsize=14)

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Display the plot
plt.tight_layout()
plt.show()
```

In London, only 47.8% (45,932 listings) are owned by single-listing hosts, while the remaining 52.2% are held by multi-listing hosts.  
Notably, hosts with 10 or more listings account for 20.8% (20,038 listings) of the total.

**conclusion:**

1. Prevalence of multi-listing hosts: more than half of all listings owned by multi-listing hosts, indicating that multi-listing is common in london.
2. Professional landlords: hosts who owned 10+ listings owned more than one fifth listings, suggesting a significant presence of professional landlords in the market.

#### 6.1.2 Changes in the number of landlords and renters over the years

```{python}
# --- Count the number of new host IDs added each year ---
new_hosts_per_year = gdf.drop_duplicates('host_id').groupby('year').size()

# --- Count the cumulative number of host IDs each year ---
year_range = range(int(gdf['year'].min()), int(gdf['year'].max()) + 1)
id_per_year = [gdf[gdf['year'] <= year].shape[0] for year in year_range]

# Create a DataFrame to display results
result_df = pd.DataFrame({
    'Year': year_range,
    'New Hosts': new_hosts_per_year.reindex(year_range, fill_value=0),
    'Total Listings Count': id_per_year
})

# Plotting the data
fig, ax1 = plt.subplots(figsize=(20, 12))  # Greatly increased figure size

# --- Plot the number of new hosts added each year (bar chart) ---
bars = ax1.bar(result_df['Year'], result_df['New Hosts'], color='#6baed6', label='New Hosts per Year', alpha=0.8)
ax1.set_xlabel('Year', fontsize=24, fontweight='bold')  # Significantly larger font size
ax1.set_ylabel('New Hosts', color='#6baed6', fontsize=24, fontweight='bold')
ax1.tick_params(axis='y', labelcolor='#6baed6', labelsize=22)
ax1.tick_params(axis='x', labelsize=22)

# Annotate bar heights with values
for bar in bars:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width() / 2, height, f'{int(height)}', 
             ha='center', va='bottom', fontsize=18, color='#6baed6')  # Slightly smaller font for annotations

# --- Add a second y-axis for cumulative listings (line chart) ---
ax2 = ax1.twinx()
lines = ax2.plot(result_df['Year'], result_df['Total Listings Count'], color='#fd8d3c', marker='o', markersize=10, label='Total Listings Count')
ax2.set_ylabel('Listings Count', color='#e6550d', fontsize=24, fontweight='bold')
ax2.tick_params(axis='y', labelcolor='#e6550d', labelsize=22)

# Annotate line chart points with values
for x, y in zip(result_df['Year'], result_df['Total Listings Count']):
    ax2.text(x, y + 500, f'{int(y)}', ha='center', va='bottom', fontsize=18, color='#e6550d')  # Slightly smaller font for annotations

# --- Set x-axis ticks to display years as integers ---
plt.xticks(ticks=result_df['Year'], labels=result_df['Year'], rotation=45, fontsize=22)

# --- Add a legend ---
fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.95), fontsize=20)

# --- Set the title of the plot ---
plt.title('Yearly New Hosts and Listings on Airbnb in London', fontsize=30, fontweight='bold')

# --- Display the plot ---
plt.tight_layout()
plt.show()
```

* Based on Airbnb's dataset for London, the whole story started in 2008 and the growth in hosts and listings was slow between 2008 and 2010, **accelerating sharply from 2011 to 2016**. The peak occurred in 2015, with 8,939 new hosts, while 2014 and 2016 saw increases of over 7,000 hosts each. By 2016, total listings neared 50,000. 

* However, **growth slowed in subsequent years**, with 2020 and 2021 adding only around 1,600 hosts and 3,000 listings annually—nearly half the growth seen in 2019—largely due to the pandemic’s impact on the rental market. From 2022 to 2024, post-pandemic recovery is evident, but growth remains far below peak levels.

### 6.2 Analysis of property
#### 6.2.1 Distribution of room types of property

```{python}
#| output: false

# Count the number of each property type
property_counts = gdf['property_type'].value_counts()

# Filter out the categories with frequency greater than 200
filtered_property_counts = property_counts[property_counts > 200]

# Calculate the percentage for each category
total_count = property_counts.sum()  # 总的数量
filtered_percentage = (filtered_property_counts / total_count) * 100

# Draw a bar chart
plt.figure(figsize=(10, 6))
bars = filtered_property_counts.plot(kind='bar', color='skyblue')

# Add title and tag
plt.title('Distribution of Property Types (Count > 200)')
plt.xlabel('Property Type')
plt.ylabel('Count')

for bar, count, percentage in zip(bars.patches, filtered_property_counts, filtered_percentage):
    height = bar.get_height()
    # Show count at the top and percentage at the bottom
    plt.text(bar.get_x() + bar.get_width() / 2, height + 10, f'{count}', 
             ha='center', va='bottom', fontsize=9)
    plt.text(bar.get_x() + bar.get_width() / 2, height - 10, f'({percentage:.1f}%)', 
             ha='center', va='top', fontsize=9)

plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()
```

```{python}
#| output: false

room_type_counts = gdf['room_type'].value_counts()
room_type_percentage = gdf['room_type'].value_counts(normalize=True) * 100

room_type_stats = pd.DataFrame({
    'Count': room_type_counts,
    'Percentage': room_type_percentage
})

room_type_stats
```

***Room type of property*** is divided into four categories.

- Entire home/apt: 63.8%
- Private room: 35.6%
- Shared room: 0.45%
- Hotel room: 0.2%

**Conclusion:**

1. The high proportion of entire homes/apt indicates that many guests prefer independent accommodations for greater privacy and autonomy.  
    This aligns with a broader shift in tourism, where more visitors are opting for alternative lodging options instead of traditional hotels to enjoy a more spacious and private environment [@zervas2017rise].
2. The 35.6% share of private rooms suggests that some guests are still willing to choose more affordable accommodations, even if it means sharing common spaces. These listings cater to budget-conscious travelers.
3. The low percentages of shared rooms and hotel rooms indicate that Airbnb's core market in London, a well-established market, tends to favor more private lodging options.

#### 6.2.2 Distribution of Minimum Nights for renting property

```{python}
#| output: false

# Create a subplot
fig, ax = plt.subplots(figsize=(8, 4))  # Single subplot

# Set values greater than 35 to 35
gdf['minimum_nights_clipped'] = np.where(gdf['minimum_nights'] > 35, 35, gdf['minimum_nights'])
bins = list(range(1, 37))  # Bins from 1 to 35, with 35+ combined into one bin

# Plot the histogram for 'minimum_nights' column
ax.hist(gdf['minimum_nights_clipped'], bins=bins, color='lightblue', edgecolor='black')

# Add a vertical line for the threshold (at 30 days)
ax.axvline(x=30, color='black', linestyle='--', linewidth=1.5, label='STR Threshold (30 days)')

# Calculate the proportion below the 30-day threshold
threshold = 30
below_threshold = gdf[gdf['minimum_nights'] < threshold].shape[0]  # Count of listings below 30 days
total_entries = gdf.shape[0]  # Total count of listings
percentage_below = (below_threshold / total_entries) * 100

# Display the percentage information on the graph
ax.text(18, ax.get_ylim()[1] * 0.8,  # Set position
        f'{below_threshold} listings ({percentage_below:.1f}%) below 30 nights',
        fontsize=12, color='black', ha='center', bbox=dict(facecolor='white', alpha=0.6))

# Set the title and labels
ax.set_title('Distribution of Minimum Nights', fontsize=15)
ax.set_xlabel('Minimum Nights')
ax.set_ylabel('Count')
ax.legend()

# Set x-axis ticks and labels
x_ticks = list(range(1, 8)) + list(range(10, 36, 5))  # Show daily ticks for the first 7 days, and every 5 days from 10 to 35
x_labels = [str(i) for i in x_ticks[:-1]] + ['35+']  # Define tick labels
ax.set_xticks(x_ticks)
ax.set_xticklabels(x_labels)

# Display the plot
plt.tight_layout()
plt.show()
```

```{python}
#| output: false

#Percentage of listings for which minimum nights are less than 7.
gdf[gdf['minimum_nights']<=7].shape[0] / total_entries # 0.9232768749285172
```

Based on the dataset, 93550 listings have a minimum night stay of less than the STR threshold (30 days), making up 97.3% of the total. Additionally, listings with a minimum stay of less than 7 days account for 92.3% of the total. 
**The London rental market on airbnb is dominated by short-term rentals.**

Chaudhary had illustrated some drawbacks of short term renting [@chaudhary2021effects].

1. **Reduced long-term housing supply**: Due to higher profits from short-term rentals (e.g., Airbnb), many landlords prioritize short-term leases over long-term rentals, exacerbating London's housing crisis and driving up rents, especially for low- and middle-income residents.
2. **Community impacts**: A high volume of short-term rentals can disrupt neighborhoods, increasing noise and tourist traffic, making communities less appealing for long-term residents and undermining stability and safety.

## 

## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London?

### 7.3 The Impact of Airbnb on London Neighborhoods: A K-Means Geodemographic Classification

- This analysis categorizes London neighborhoods into distinct groups based on how they have, or have not, been impacted by Airbnb. Using K-means clustering and relevant socio-economic and geographic indicators, we developed profiles to assist policymakers in understanding the diversity of environments where Airbnb operates. 

#### 7.3.1 Data processing

##### Variables that need to be processed :
  - Airbnb Average Price per Night: Reflects the economics of the short-term rental market.
  - Airbnb Density: shows how well Airbnb's are distributed in a given neighborhood.
  - Changes in Social Listing Prices Over a Five-Year Period: Indirectly reflects Airbnb's potential impact on the local real estate market.
  - Population Density: Indicates the size and density of a community's population.
  - Income Deprivation Index: Higher values indicate greater income deprivation, which may be
  - correlate with the economic status and social vulnerability of the community
  - Hotel Density: measures the competitive environment for traditional lodging establishments.
  - Attraction Density: Reflects the community's tourist appeal and potential demand for Airbnb's.
  - Public Transportation Accessibility: Demonstrates the community's accessibility to tourists.

  1. Calculate the density of tourist attractions in each LSOA in London

## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London?

### 7.3 The Impact of Airbnb on London Neighborhoods: A K-Means Geodemographic Classification

#### 7.3.1 Data processing

 *key variables*
  - Airbnb Average Price per Night: Reflects the economics of the short-term rental market.
  - Airbnb Density: shows how well Airbnb's are distributed in a given neighborhood.
  - Changes in Social Listing Prices Over a Five-Year Period: Indirectly reflects Airbnb's potential impact on the local real estate market.
  - Population Density: Indicates the size and density of a community's population.
  - Income Deprivation Index: Higher values indicate greater income deprivation, which may be
  - correlate with the economic status and social vulnerability of the community
  - Hotel Density: measures the competitive environment for traditional lodging establishments.
  - Attraction Density: Reflects the community's tourist appeal and potential demand for Airbnb's.
  - Public Transportation Accessibility: Demonstrates the community's accessibility to tourists.

*Variables that need to be processed :*

1. Calculate the density of tourist attractions in each LSOA in London

## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London?

### 7.3 The Impact of Airbnb on London Neighborhoods: A K-Means Geodemographic Classification

#### 7.3.1 Data processing

*key variables*
 - Airbnb Average Price per Night: Reflects the economics of the short-term rental market.
  - Airbnb Density: shows how well Airbnb's are distributed in a given neighborhood.
  - Changes in Social Listing Prices Over a Five-Year Period: Indirectly reflects Airbnb's potential impact on the local real estate market.
  - Population Density: Indicates the size and density of a community's population.
  - Income Deprivation Index: Higher values indicate greater income deprivation, which may be
  - correlate with the economic status and social vulnerability of the community
  - Hotel Density: measures the competitive environment for traditional lodging establishments.
  - Attraction Density: Reflects the community's tourist appeal and potential demand for Airbnb's.
  - Public Transportation Accessibility: Demonstrates the community's accessibility to tourists.

*Variables that need to be processed :*

1. Calculate the density of tourist attractions in each LSOA in London

```{python}
#| output: false
!pip install --upgrade osmnx
import osmnx as ox
print(ox.__version__)  # Should be 1.2.2 or newer
```

```{python}
#| output: false
import geopandas as gpd
import pandas as pd

#  Load London LSOA boundary data
lsoa_boundaries = gpd.read_file("data/London/LSOA_2011_London_gen_MHW.shp")

# Retrieve tourist attraction data for London
# Use OSMnx to extract nodes tagged as `tourism=attraction`
tourist_attractions = ox.features_from_place(
    "London, England", tags={"tourism": "attraction"}
)

# Ensure the coordinate reference system (CRS) is consistent
tourist_attractions = tourist_attractions.to_crs(lsoa_boundaries.crs)

# : Count the number of tourist attractions within each LSOA
lsoa_boundaries["tourist_attraction_count"] = lsoa_boundaries.apply(
    lambda row: tourist_attractions.within(row.geometry).sum(), axis=1
)

#  Calculate the density of tourist attractions (per square kilometer)
# Transform to an equal-area projection to compute accurate area
lsoa_boundaries = lsoa_boundaries.to_crs({"proj": "cea"})
lsoa_boundaries["area_km2"] = lsoa_boundaries.geometry.area / 1e6  # Convert to square kilometers

# Tourist attraction density = count / area
lsoa_boundaries["tourist_attraction_density"] = (
    lsoa_boundaries["tourist_attraction_count"] / lsoa_boundaries["area_km2"]
)

#  Export results to a CSV file
# Use 'LSOA11CD' as the LSOA code column
output_path = "data/london_lsoa_tourist_density.csv"
output_csv = lsoa_boundaries[["LSOA11CD", "tourist_attraction_count", "tourist_attraction_density"]]
output_csv.to_csv(output_path, index=False)

print(f"Tourist attraction density has been saved to {output_path}")
```

 2. Calculate hotel density for each LSOA unit

```{python}
#| output: false
from shapely.geometry import Point

# 1. Load LSOA boundary data
lsoa_shp_path = "data/London/LSOA_2011_London_gen_MHW.shp"
lsoa_data = gpd.read_file(lsoa_shp_path)

# (EPSG:27700 - British National Grid)
lsoa_data = lsoa_data.to_crs("EPSG:27700")
lsoa_data['area_km2'] = lsoa_data['geometry'].area / 10**6

lsoa_data = lsoa_data.to_crs("EPSG:4326")

# 2. Get the London boundary 
london_boundary = lsoa_data.dissolve().geometry.iloc[0]

# 3. Fetch hotel data from OpenStreetMap
hotels = ox.features_from_polygon(london_boundary, tags={"tourism": "hotel"}) 

# Convert the hotel data into a GeoDataFrame and retain only geometry
hotels_gdf = gpd.GeoDataFrame(hotels[['geometry', 'name']].dropna(subset=['geometry']), crs="EPSG:4326")

# 4. Spatially join hotels to their respective LSOA units
hotels_in_lsoa = gpd.sjoin(hotels_gdf, lsoa_data, how="inner", predicate="within")

# Count the number of hotels per LSOA
hotel_counts = hotels_in_lsoa.groupby('LSOA11CD').size().reset_index(name='hotel_count')

# 5. Calculate hotel density (number of hotels per square kilometer)
lsoa_data = lsoa_data.merge(hotel_counts, on='LSOA11CD', how='left')
lsoa_data['hotel_count'] = lsoa_data['hotel_count'].fillna(0)
lsoa_data['hotel_density'] = lsoa_data['hotel_count'] / lsoa_data['area_km2']

# 6. Save results to a CSV file
output_csv_path = "data/lsoa_hotel_density.csv"
lsoa_data[['LSOA11CD', 'hotel_count', 'area_km2', 'hotel_density']].to_csv(output_csv_path, index=False)

print(f"Hotel density data saved to {output_csv_path}")
```

 3. Calculate five-year house price changes for each LSOA unit

```{python}
#| output: false
file_path_xlsx = "data/LSOA_Median_properties_price.xlsx"

# Load the data
data_xlsx = pd.read_excel(file_path_xlsx, engine='openpyxl')

# List of all London boroughs including the City of London
london_boroughs = [
    "City of London", "Barking and Dagenham", "Barnet", "Bexley", "Brent", "Bromley", "Camden",
    "Croydon", "Ealing", "Enfield", "Greenwich", "Hackney", "Hammersmith and Fulham", "Haringey",
    "Harrow", "Havering", "Hillingdon", "Hounslow", "Islington", "Kensington and Chelsea",
    "Kingston upon Thames", "Lambeth", "Lewisham", "Merton", "Newham", "Redbridge",
    "Richmond upon Thames", "Southwark", "Sutton", "Tower Hamlets", "Waltham Forest", "Wandsworth", "Westminster"
]

# Filter the data for London based on the local authority name
london_data = data_xlsx[data_xlsx['Local authority name'].isin(london_boroughs)]

# Save the filtered data to a CSV file
output_csv_path = "data/London_Housing_Data.csv"
london_data.to_csv(output_csv_path, index=False)

print(f"Filtered London data has been saved to {output_csv_path}")
```

```{python}
#| output: false

# Load the data
data = pd.read_csv('data/London_Housing_Data.csv')

# Strip any leading/trailing spaces in the column names
data.columns = data.columns.str.strip()

# Convert 'Year ending Mar 2023' and 'Year ending Dec 2018' to numeric (if they are not already)
data['Year ending Mar 2023'] = pd.to_numeric(data['Year ending Mar 2023'], errors='coerce')
data['Year ending Dec 2018'] = pd.to_numeric(data['Year ending Dec 2018'], errors='coerce')

# Check for missing values in the year columns after conversion
missing_values = data[['Year ending Mar 2023', 'Year ending Dec 2018']].isnull().sum()
print(f"Missing values in Year columns:\n{missing_values}")

# Create a copy of the data to avoid SettingWithCopyWarning
data_clean = data.copy()

# Drop rows with missing values for price calculations (only for the year columns)
data_clean = data_clean.dropna(subset=['Year ending Mar 2023', 'Year ending Dec 2018'])

# Calculate the price change rate between 'Year ending Mar 2023' and 'Year ending Dec 2018'
# Use .loc to avoid SettingWithCopyWarning
data_clean.loc[:, 'Price_Change'] = ((data_clean['Year ending Mar 2023'] - data_clean['Year ending Dec 2018']) / data_clean['Year ending Dec 2018'] * 100)

# Merge the price change back with the original data (using LSOA_code as the key)
data_with_change = pd.merge(data, data_clean[['LSOA code', 'Price_Change']], on='LSOA code', how='left')

# Ensure data is sorted by LSOA code
data_with_change_sorted = data_with_change.sort_values(by=['LSOA code'])

# Save the result to a new CSV file
data_with_change_sorted[['LSOA code', 'Price_Change']].to_csv('data/LSOA_London_houseprice_change.csv', index=False)

print("House price change data saved to 'LSOA_London_houseprice_change.csv'")
```

```{python}
#| output: false
# Define paths for the input Excel files and output CSV files
input_file_imd = "data/LSOA_IMD_london.xlsx"
output_file_imd = "data/LSOA_IMD_london.csv"

input_file_population = "data/LSOA_population_density.xlsx"
output_file_population = "data/LSOA_population_density.csv"

# Function to convert an Excel file to a CSV file
def convert_excel_to_csv(input_path, output_path):
    # Read the Excel file
    data = pd.read_excel(input_path)
    # Save the data to a CSV file
    data.to_csv(output_path, index=False)
    print(f"Converted {input_path} to {output_path}")

# Convert LSOA_IMD_london.xlsx to LSOA_IMD_london.csv
convert_excel_to_csv(input_file_imd, output_file_imd)

# Convert LSOA_population_density.xlsx to LSOA_population_density.csv
convert_excel_to_csv(input_file_population, output_file_population)
```

 4. Read in and process Airbnb data

```{python}
#| output: false
# read in Airbnb data
airbnb_data = pd.read_csv('data/listings.csv.gz', compression='gzip', low_memory=False)
```

```{python}
#| output: false
import geopandas as gpd
import pandas as pd
from shapely.geometry import Point

# Filter out only the columns we need
columns_to_keep = ['id', 'property_type', 'room_type', 'price', 'review_scores_rating', 'review_scores_value', 'longitude', 'latitude']
airbnb_filtered = airbnb_data[columns_to_keep].copy() 

airbnb_filtered.loc[:, 'geometry'] = airbnb_filtered.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)

# Create a GeoDataFrame
airbnb_gdf = gpd.GeoDataFrame(airbnb_filtered, geometry='geometry', crs="EPSG:4326")  # Ensure CRS is WGS84

airbnb_filtered.to_csv('data/filtered_airbnb_data.csv', index=False)

airbnb_gdf.to_file('data/filtered_airbnb_data.geojson', driver='GeoJSON')

# Print the cleaned data
print(airbnb_gdf.head())
```

 5. Calculate Airbnb Density and Average Price per Night for LSOA Units

```{python}
#| output: false
from shapely.geometry import Point

# Load the LSOA boundary data
lsoa_shp_path = 'data/London/LSOA_2011_London_gen_MHW.shp'
lsoa_data = gpd.read_file(lsoa_shp_path)

# Use the projected coordinate system to calculate the area (EPSG:27700 - British National Grid)
lsoa_data = lsoa_data.to_crs("EPSG:27700")
lsoa_data['area_km2'] = lsoa_data.geometry.area / 10**6  # Convert area from square meters to square kilometers

# Reproject back to WGS84 for subsequent spatial operations
lsoa_data = lsoa_data.to_crs("EPSG:4326")

# Load the Airbnb data with latitude and longitude
airbnb_df = pd.read_csv('data/filtered_airbnb_data.csv')

# Ensure the Airbnb data has latitude and longitude columns and create geometries
airbnb_df['geometry'] = airbnb_df.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)

# Convert DataFrame to GeoDataFrame
airbnb_gdf = gpd.GeoDataFrame(airbnb_df, geometry='geometry', crs="EPSG:4326")

# Spatial join to get the Airbnb points within each LSOA
airbnb_in_lsoa = gpd.sjoin(airbnb_gdf, lsoa_data, how="inner", predicate="within")

# Clean the price column
airbnb_in_lsoa['price'] = airbnb_in_lsoa['price'].replace({'\$': '', ',': ''}, regex=True).astype(float)

# Calculate the average price for each LSOA
average_price = airbnb_in_lsoa.groupby('LSOA11CD')['price'].mean().reset_index()
average_price.rename(columns={'price': 'average_price'}, inplace=True)

# Calculate the number of Airbnb listings in each LSOA
airbnb_density = airbnb_in_lsoa.groupby('LSOA11CD')['price'].count().reset_index(name='airbnb_count')

# Merge average price and Airbnb count with LSOA data
lsoa_data = lsoa_data.merge(average_price, on='LSOA11CD', how='left')
lsoa_data = lsoa_data.merge(airbnb_density, on='LSOA11CD', how='left')

# Calculate Airbnb density (number of listings per square kilometer)
lsoa_data['airbnb_density'] = lsoa_data['airbnb_count'] / lsoa_data['area_km2']

# Display the result
print(lsoa_data[['LSOA11CD', 'average_price', 'airbnb_count', 'airbnb_density']].head())
```

 6. Merge all data

```{python}
#| output: false
# Load all the datasets
airbnb_df = pd.read_csv('data/LSOA_airbnb_summary.csv')
houseprice_df = pd.read_csv('data/LSOA_London_houseprice_change.csv')
pop_density_df = pd.read_csv('data/LSOA_population_density.csv')
imd_df = pd.read_csv('data/LSOA_IMD_london.csv')
hotel_density_df = pd.read_csv('data/lsoa_hotel_density.csv')
tourist_density_df = pd.read_csv('data/london_lsoa_tourist_density.csv')
avptai_df = pd.read_csv('data/LSOA_AvPTAI.csv')

#  Merge the datasets based on the correct columns
merged_data = airbnb_df.merge(houseprice_df, left_on='LSOA11CD', right_on='LSOA code', how='left')
merged_data = merged_data.merge(pop_density_df, left_on='LSOA11CD', right_on='LSOA Code', how='left')
merged_data = merged_data.merge(imd_df, left_on='LSOA11CD', right_on='LSOA code (2011)', how='left')
merged_data = merged_data.merge(hotel_density_df, on='LSOA11CD', how='left')
merged_data = merged_data.merge(tourist_density_df, on='LSOA11CD', how='left')
merged_data = merged_data.merge(avptai_df, left_on='LSOA11CD', right_on='LSOA2011', how='left')

# Select only the relevant columns
final_data = merged_data[['LSOA11CD', 'average_price', 'airbnb_density', 'Price_Change', 
                          'People per Sq Km', 'Income Score (rate)', 'hotel_density', 
                          'tourist_attraction_density', 'AvPTAI2015']]

# Check the first few rows of the final data
print(final_data.head())

# Optionally, save the final merged data to a CSV file
final_data.to_csv('data/final_LSOA_data.csv', index=False)
```

 7. Check and clean the data  

```{python}
#| output: false
# Step 1: Check the column names for consistency
print(final_data.columns)

# Step 2: Strip any leading/trailing spaces from column names
final_data.columns = final_data.columns.str.strip()

# Step 3: Check for missing values
print(final_data.isnull().sum())

# Step 4: Fill missing values for numerical columns using the median, using `.loc` to avoid SettingWithCopyWarning
numerical_columns = ['average_price', 'airbnb_density', 'Price_Change', 'People per Sq Km', 
                     'Income Score (rate)', 'hotel_density', 'tourist_attraction_density', 'AvPTAI2015']

for col in numerical_columns:
    if col in final_data.columns:
        final_data.loc[:, col] = final_data[col].fillna(final_data[col].median())

# Step 5: Check if any missing values remain
print(final_data.isnull().sum())
```

#### 7.3.2 Standardization (Z-score scaling)

```{python}
#| output: false
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Print the column names
print(final_data.columns)

# Dynamically create numerical_columns
numerical_columns = [col for col in ['average_price', 'airbnb_density', 'price_change', 
                     'people per sq km', 'income score (rate)', 
                     'hotel_density', 'tourist_attraction_density', 'avptai2015'] if col in final_data.columns]

print("Columns to be standardized:", numerical_columns)

# Initialize StandardScaler
scaler = StandardScaler()

# Safely apply standardization using .loc
final_data.loc[:, numerical_columns] = scaler.fit_transform(final_data[numerical_columns])

# Check the result
print(final_data.head())
```

#### 7.3.3 Elbow Method to caculate the k value

```{python}
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import numpy as np
import pandas as pd

# Set random seeds to ensure reproducibility
random_state = 42
np.random.seed(random_state)

# Create a copy of the dataframe (ensure the data is stable and sorted)
final_data = final_data.copy()
final_data = final_data.sort_values(by=final_data.columns.tolist()).reset_index(drop=True)

# Select relevant columns (excluding 'LSOA11CD' for clustering)
clustering_data = final_data.drop(columns=['LSOA11CD'])

# Standardize the data (Z-score scaling)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(clustering_data)

# Elbow Method for Optimal K
plt.figure(figsize=(8, 5))
inertia = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)  # Fix n_init
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

# Plot Elbow Method
plt.plot(K_range, inertia, marker='o')
plt.title("Elbow Method for Optimal K")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Inertia")
plt.grid(True)
plt.show()
```

 - Based on the information provided in the graphs, the optimal number of clusters (K) appears to be 3.

#### 7.3.4 K-means Clustering

```{python}
#| output: false
import numpy as np
from sklearn.cluster import KMeans

# Set the random state for reproducibility
random_state = 42
np.random.seed(random_state)

# Assume final_data is your dataframe and X_scaled is your scaled feature data
# Perform K-Means clustering with fixed random state, n_init, and initial centers

optimal_k = 4  # Optimal number of clusters

# Fixing the initial centroids using k-means++ initialization
# k-means++ is deterministic when the random state is set
kmeans = KMeans(n_clusters=optimal_k, init='k-means++', n_init=10, random_state=random_state)

# Fit the model and predict clusters
final_data['Cluster'] = kmeans.fit_predict(X_scaled)

# Display clustering results
print("Clustering results:")
print(final_data[['LSOA11CD', 'Cluster']].head())

# Analyze the mean characteristics of each cluster (numerical features only)
numerical_columns = [col for col in final_data.columns if col != 'LSOA11CD' and col != 'Cluster']
cluster_summary = final_data.groupby('Cluster')[numerical_columns].mean()
print("\nCluster summary (mean values of features):")
print(cluster_summary)

# Count the number of neighborhoods in each cluster
cluster_counts = final_data['Cluster'].value_counts()
print("\nNumber of neighborhoods in each cluster:")
print(cluster_counts)

# Save the clustering results to a CSV file
final_data.to_csv('data/london_airbnb_clustering_results.csv', index=False)
print("\nClustering results have been saved to 'data/london_airbnb_clustering_results.csv'.")
```

**Results of K-means clustering**

1. Cluster 0: "Low Impact — Stable Residential Areas"
- Lowest Airbnb Density (-0.42) and Hotel Density (-0.13) indicate minimal tourism and Airbnb presence.
- Highest Price Change (19.39) reflects significant housing market growth, possibly driven by local development or gentrification.
- Low Population Density (6,688 people/km²) and Poor Public Transport Access (7.98) make these areas less attractive to short-term rentals.
- Moderate Income Deprivation (0.11) suggests economic vulnerability. 

2. Cluster 1: "Moderate Impact — Balanced Zones"
- Moderate Airbnb Density (0.44) and Hotel Density (-0.06) indicate balanced tourism and residential dynamics.
- Moderate Price Change (10.75) and Population Density (16,761 people/km²) suggest a stable housing market with mixed use.
- Moderate Public Transport Access (19.40) supports tourism without overwhelming local infrastructure.
- Higher Income Deprivation (0.19) reflects economic challenges despite balanced Airbnb activity.

3. Cluster 2: "Emerging Impact — High Price Pressure Areas"
- Unusually High Average Price (18.90) indicates significant economic pressures on the housing market.
- Moderate Airbnb Density (0.48) and Tourist Attraction Density (0.19) reflect emerging tourism activity.
- High Population Density (17,579 people/km²) combined with Good Public Transport Access (30.45) supports growth in short-term rentals.
- Moderate Income Deprivation (0.16) suggests economic tension within these areas.

4. Cluster 3:"High Impact — Tourist Hotspots"
- Highest Airbnb Density (3.51) and Hotel Density (3.11) mark these areas as major tourism accommodation hubs.
- Highest Average Price (0.54) suggests strong profitability for short-term rentals.
- High Tourist Attraction Density (1.63) and Population Density (20,976 people/km²) highlight their appeal to visitors.
- Convenient Public Transport Access (53.91) facilitates heavy visitor traffic.

#### 7.3.5 Plot K-Means Clustering Result

```{python}
import matplotlib.pyplot as plt
import geopandas as gpd
import pandas as pd
from matplotlib.patches import Patch
from matplotlib.legend_handler import HandlerPatch
from IPython.display import Image  # 用于显示保存的图片

# Load LSOA shapefile
shapefile_path = 'data/London/LSOA_2011_London_gen_MHW.shp'
lsoa_gdf = gpd.read_file(shapefile_path)

# Load clustering results
clustering_results = pd.read_csv('data/london_airbnb_clustering_results.csv')
clustering_results.rename(columns={'lsoa11cd': 'LSOA11CD'}, inplace=True)
lsoa_gdf = lsoa_gdf.merge(clustering_results[['LSOA11CD', 'Cluster']], on='LSOA11CD', how='left')

# Ensure the CRS matches
lsoa_gdf = lsoa_gdf.to_crs("EPSG:4326")

# Create a map plot
fig, ax = plt.subplots(figsize=(12, 12))

# Define softer and more pastel cluster colors, with cluster 0 as a transparent light blue
cluster_colors = {
    0: (0.6, 0.8, 1.0, 0.2),  # Transparent light blue (R, G, B, Alpha)
    1: '#FCD34D',  # Soft peach
    2: '#93C5FD',  # Soft blue
    3: '#0284C7'   # Vibrant blue (to make cluster 3 more prominent)
}

# Custom legend handler for PatchCollection
class HandlerColoredPatch(HandlerPatch):
    def create_artists(self, legend, orig_handle,
                       xdescent, ydescent, width, height, fontsize, trans):
        return [Patch(facecolor=orig_handle.get_facecolor(), edgecolor='gray', linewidth=0.3)]

# Plot LSOA regions colored by cluster
for cluster, color in cluster_colors.items():
    if isinstance(color, tuple):  # If the color is in RGBA format (transparent)
        lsoa_gdf[lsoa_gdf['Cluster'] == cluster].plot(
            ax=ax, color=color, edgecolor='gray', linewidth=0.3, label=f'Cluster {cluster}'
        )
    else:  # For solid colors
        lsoa_gdf[lsoa_gdf['Cluster'] == cluster].plot(
            ax=ax, color=color, edgecolor='gray', linewidth=0.3, label=f'Cluster {cluster}'
        )

# Layout and legend styling
ax.set_title("K-Means Clustering Results by LSOA", fontsize=16, fontweight='light')
ax.set_xlabel("Longitude", fontsize=12)
ax.set_ylabel("Latitude", fontsize=12)

# Adjust the legend to handle custom colors
legend_labels = [Patch(facecolor=color, edgecolor='gray', label=f'Cluster {cluster}') for cluster, color in cluster_colors.items()]
ax.legend(handles=legend_labels, title="Clusters", loc='upper right', fontsize=12)

# Remove axis for a cleaner map
ax.set_axis_off()

# Save the map to a file
output_image_path = 'data/london_airbnb_clusters_map.png'
plt.tight_layout()
plt.savefig(output_image_path)
plt.show()
```

- *From the map:*
- Cluster 0 dominates the outer boroughs of London and peripheral areas.
- Cluster 1, which is Small, isolated neighborhoods near central areas showing signs of Airbnb growth.
- Cluster 2 is widely distributed across inner London and intermediate zones surrounding central areas.It forms a ring around the core city center, particularly dense in the inner boroughs like Hackney, Islington, and Camden.
- Cluster 3, is concentrated in central London, including areas like Westminster, the City of London, and parts of Southwark.
  

#### 7.3.6 Conclusion on Airbnb Impact and Policy Recommendations:

1. Low Impact — Stable Residential Areas:
  - Airbnb Impact:These neighborhoods are largely residential with minimal tourism influence but are at risk of future Airbnb expansion due to rising housing prices.
  - Policy Recommendations: Proactively monitor Airbnb growth. Implement measures to prevent housing market disruption, such as rental density caps.

2. Emerging Airbnb Areas — “Expansion Zones”:
  - Airbnb Impact: These neighborhoods have a moderate Airbnb presence that coexists with local housing needs.
  - Policy Recommendations: Encourage sustainable tourism while safeguarding affordable housing. Adopt policies that prevent excessive short-term rental concentration.
 

3. High Impact — Tourist Hotspots:
 - Airbnb Impact: These areas are already heavily impacted by Airbnb activity, creating pressure on housing availability and affordability.
 - Policy Recommendations: Introduce stricter regulations, such as rental caps, zoning restrictions, and taxation on short-term rentals to protect long-term residents.

4. Emerging Impact — High Price Pressure Areas:
- Airbnb Impact: These neighborhoods are emerging hotspots, where Airbnb activity and housing pressures are growing.
- Policy Recommendations: Implement early intervention strategies to balance tourism growth and housing stability. Monitor housing price trends closely.

## Sustainable Authorship Tools

Using the Terminal in Docker, you compile the Quarto report using `quarto render <group_submission_file>.qmd`.

Your QMD file should automatically download your BibTeX and CLS files and any other required files. If this is done right after library loading then the entire report should output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References
