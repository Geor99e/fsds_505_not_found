---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Group Name's Group Project
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python (base)
    language: python
    name: base
---

```{python}
#| echo: false

'''
explanation: '| echo: false' means not showing source code, and 
            '| output: false' means not showing output of that code chunk (print, plt.show(), etc.)
And these should be placed in the first row of each code chunk.
'''

# import library used in this qmd file
import os
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt

from requests import get
import requests
from urllib.parse import urlparse
from functools import wraps
import zipfile
import shutil
```

```{python}
#| output: false

# Download the whole data file from remote repository:

# configuration parameter
GITHUB_ZIP_URL = "https://github.com/YYY677/fsds_505_not_found/archive/refs/heads/master.zip"
TARGET_FOLDER = "fsds_505_not_found-master/data"  # GitHub path
LOCAL_FOLDER = "./data"  # local file path

try:
    # check if the folder already exists
    if os.path.exists(LOCAL_FOLDER):
        print(f"The folder '{LOCAL_FOLDER}' already exists locally.")
    else:
        # download zip file from GitHub
        zip_path = "repo.zip"
        print("Downloading the repository ZIP file...")
        try:
            response = requests.get(GITHUB_ZIP_URL, timeout=30)
            response.raise_for_status()  # Raise an exception for HTTP errors
        except requests.exceptions.RequestException as e:
            print(f"Error downloading the ZIP file: {e}")
            exit(1)

        # write the ZIP file locally
        try:
            with open(zip_path, "wb") as f:
                f.write(response.content)
        except IOError as e:
            print(f"Error saving the ZIP file: {e}")
            exit(1)

        # extract the ZIP file
        print("Extracting the ZIP file...")
        try:
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(".")
        except zipfile.BadZipFile as e:
            print(f"Error extracting the ZIP file: {e}")
            os.remove(zip_path)
            exit(1)

        # move the target folder to the desired location
        print("Moving the target folder to the desired location...")
        try:
            shutil.move(TARGET_FOLDER, LOCAL_FOLDER)
        except FileNotFoundError as e:
            print(f"Error moving the folder: {e}")
            exit(1)
        except shutil.Error as e:
            print(f"Error during folder move: {e}")
            exit(1)

        # clean up temporary files
        print("Cleaning up temporary files...")
        try:
            os.remove(zip_path)
            shutil.rmtree("fsds_505_not_found-master")
        except Exception as e:
            print(f"Error during cleanup: {e}")
            # Not exiting here as the main task is complete

        print(f"The folder has been successfully downloaded to '{LOCAL_FOLDER}'.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")
```

```{python}
#| output: false

# This chunk is to down seperate files from remote repo:

def download_file(url, save_path):
    """
    Downloads a file from the given URL and saves it to the specified local path.
    """
    response = requests.get(url)
    if response.status_code == 200:
        with open(save_path, 'wb') as file:
            file.write(response.content)
        print(f"Download completed: {save_path}")
    else:
        print(f"Download failed with HTTP status code {response.status_code}, URL: {url}")

def check_and_download(url, local_path):
    """
    Checks if a file exists locally; if not, downloads it from the given URL.
    """
    if not os.path.exists(local_path):
        print(f"File does not exist. Downloading: {local_path}")
        download_file(url, local_path)
    else:
        print(f"File already exists: {local_path}")

# URLs and corresponding local save paths
url_csl = 'https://raw.githubusercontent.com/YYY677/fsds_505_not_found/master/harvard-cite-them-right.csl'
url_bib = 'https://raw.githubusercontent.com/YYY677/fsds_505_not_found/master/bio.bib'

# Specify local file paths
local_csl = "harvard-cite-them-right.csl"
local_bib = "bio.bib"

# Check and download the files
check_and_download(url_csl, local_csl)
check_and_download(url_bib, local_bib)
```

## Declaration of Authorship {.unnumbered .unlisted}

We, `505 not found`, pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date:

Student Numbers: 

## Brief Group Reflection

| What Went Well | What Was Challenging |
| -------------- | -------------------- |
| A              | B                    |
| C              | D                    |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

{{< pagebreak >}}

# Response to Questions

See the raw file for examples of how to hide computational output as there is code hidden here.

## 1. Who collected the InsideAirbnb data?

::: {.duedate}

Inside Airbnb data was collected by Murray Cox, a data activist and the project's founder, John Morris, the website designer and report producer, and Taylor Higgins, a master's student focusing on sustainable tourism at the Università degli Studi di Firenze.

:::

## 2. Why did they collect the InsideAirbnb data?

::: {.duedate}

The purpose of InsideAirbnb data is to provide data-driven insights into the impact of Airbnb on residential housing markets, thereby contributing to public discourse on the regulation and effects of short-term rental platforms in urban areas.

:::

## 3. How did they collect it?

::: {.duedate}

The data is collected by utilizing web scraping techniques such as self-made bots, inside Airbnb and AirDNA [@Pawlicz2021UNDERSTANDINGSR] [@Prentice2023AddressingDQ] to extract publicly available information from Airbnb’s website, focusing on various aspects of listings such as location, price, availability, and host details. This approach allows for the assembly of comprehensive datasets, which are then cleansed and organized to facilitate thorough analysis.

:::


## 4. How does the method of collection (Q3) impact the completeness and/or accuracy of the InsideAirbnb data? How well does it represent the process it seeks to study, and what wider issues does this raise?

::: {.duedate}

The data collection method used by InsideAirbnb raises data quality issues, mainly related to data incompleteness, reliance on website structure, and technical challenges. In terms of accuracy, data is automatically retrieved from the website, which possess the risk of capturing inaccurate or outdated information due to the dynamic nature of web content [@krotov2023big].

In addition, due to privacy measures, the geographic coordinates provided by Airbnb may not reflect the exact location of the listing, which adds a layer of inaccuracy. And as web scraping depends heavily on the structure of the Airbnb website. Changes to the website layout or measures to block scraping activities may disrupt data collection efforts, like Airbnb's anti-scrap measures including CAPTCHA or IP bans, pose additional challenges [@Prentice2023AddressingDQ]. This burdens data analysts by requiring them to constantly develop and maintain scraping scripts. 

In the discussion of the structure of InsideAirbnb data, it contains all aspects of the Airbnb market, including the distribution and characteristics of listings, pricing models, and the impact of Airbnb on the local housing market. And it is a relatively complete dataset and can assist with comprehensive analysis study.

Besides the accuracy concerns, the use of InsideAirbnb data raises technical, legal, and ethical issues.
Legally, as discussed in Sobel [@sobel2021new], scraping faces challenges in different jurisdictions, depending on how it intersects with privacy laws and terms of service agreements. This could affect the legality of the Inside Airbnb data collection process, especially if it violates Airbnb’s terms of service. Scraping also raises ethical issues, particularly regarding the consent of data subjects (Airbnb hosts and guests) whose information is collected without explicit permission. This raises significant privacy issues, as highlighted in the study by Xie and Karan [@xie2019consumers], where users’ awareness and concerns about how their data is used influence their privacy management behaviours.

:::

## 5. What ethical considerations does the use of the InsideAirbnb data raise? 

::: {.duedate}

The use of the InsideAirbnb database does raise several ethical considerations.

Firstly, there are issues of legal compliance. Web scraping can conflict with legal standards and ethical norms, particularly when data is collected without explicit consent, potentially leading to legal actions [@krotov2023big].

Secondly, privacy concerns for individuals must be addressed. Although the data might be publicly accessible, individuals typically do not anticipate their rental information being extensively aggregated and analyzed [@brenning2023web].

In many instances, data subjects (hosts and guests) are neither directly informed nor asked for consent when their data is scraped and analyzed. This presents a significant ethical dilemma: using their information without explicit permission, especially when such data might be utilized to draw conclusions or influence policies that could directly impact them.

Moreover, there is the issue of how policymaking might be influenced by the data. Since the scraped data can contain errors, issues with accuracy and potential misrepresentation may lead to misleading conclusions that could negatively affect Airbnb hosts, guests, and policy decisions.

Additionally, the misuse of data poses a significant ethical concern. When analyzing Inside Airbnb data, it is crucial to ensure that the data is not used for purposes unintended by the original data providers, such as market manipulation, unfair competition, or research that adversely impacts hosts and guests.

Lastly, transparency and accountability are crucial. Ethical research involving data scraping should clearly disclose its methodologies, the specific data collected, and how this data is utilized. Such transparency is especially important for accountability, particularly if the research has the potential to influence public opinion or policy [@brenning2023web].

:::



## 6. With reference to the InsideAirbnb data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and the types of properties that they list suggest about the nature of Airbnb lettings in London? 

### 6.1 Analysis of Hosts

```{python}
# read the latest airbnb listings dataset
df22 = pd.read_csv('data/listings.csv.gz', compression='gzip', low_memory=False)
```

```{python}
#| output: false

#expand count limitation of column display
pd.options.display.max_columns = 75 

# df22.info()
```

```{python}
# convert df(pandas) file to gdf(geopandas, which contains spatial column)
gdf = gpd.GeoDataFrame(df22, crs='EPSG:4326', geometry=gpd.points_from_xy(df22.longitude, df22.latitude)).to_crs(27700)

gdf = gdf[['host_since', 'host_id', 'host_listings_count', 'property_type', 'room_type', 'price', 'minimum_nights']]
# gdf.info()
```

```{python}
#| output: false

# This code chunk is about Wrangling Data

# Ensure 'host_since' is in datetime format
gdf['host_since'] = pd.to_datetime(gdf['host_since'], errors='coerce')
# Drop rows with NaT in 'host_since'
gdf = gdf.dropna(subset=['host_since'])

# Extract the year from 'host_since'
gdf['year'] = gdf['host_since'].dt.year.fillna(-1).astype(int)

# Handle missing values in 'host_listings_count'
gdf['host_listings_count'] = gdf['host_listings_count'].fillna(-1).astype(int)

# Clean 'price' column and convert it to float
gdf['price'] = gdf['price'].astype(str)
gdf['price'] = gdf['price'].str.replace('$','', regex=False).str.replace(',','', regex=False).astype(float).fillna(-1)

# Convert 'property_type' and 'room_type' to categorical data types
gdf['property_type'] = gdf['property_type'].astype('category')
gdf['room_type'] = gdf['room_type'].astype('category')

# gdf.head()
```

```{python}
#| output: false

gdf.info()
gdf['year'].min()
```

#### 6.1.1 Distribution of the Number of Listings per Host

```{python}
#| output: false

# This code chunk is to look at 'Distribution of the Number of Listings per Host'.

# Group by 'host_id' to count the number of listings per host
host_counts = gdf.groupby('host_id').size()

# Define bins and labels for the grouping
bins = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, float('inf')]  # 分组区间
labels = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10+']  # 分组标签

# Group the host counts into bins
host_counts_bins = pd.cut(host_counts, bins=bins, labels=labels, right=True)

# Count total listings per group (number * number of listings per host)
listings_per_group = host_counts.groupby(host_counts_bins).apply(lambda x: (x * x.index.to_series().value_counts()).sum())

# Calculate total number of listings
total_listings = listings_per_group.sum()
percentages = (listings_per_group / total_listings) * 100

# Plotting the bar chart
plt.figure(figsize=(8, 6))
bars = plt.bar(listings_per_group.index, listings_per_group.values, color='skyblue')

# Add labels with counts and percentages
for bar, count, pct in zip(bars, listings_per_group.values, percentages):
    plt.text(
        bar.get_x() + bar.get_width() / 2,  # x 坐标
        bar.get_height() + 0.5,  # y 坐标
        f'{count:.0f}\n({pct:.1f}%)',  # 标签内容
        ha='center', va='bottom', fontsize=10
    )

# Set the title and axis labels
plt.title('Distribution of Listings by Listings per Host', fontsize=16)
plt.xlabel('Listings per Host (Grouped)', fontsize=14)
plt.ylabel('Total Listings', fontsize=14)

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Display the plot
plt.tight_layout()
plt.show()
```

In London, only 47.8% (45,932 listings) are owned by single-listing hosts, while the remaining 52.2% are held by multi-listing hosts.  
Notably, hosts with 10 or more listings account for 20.8% (20,038 listings) of the total.

**conclusion:**

1. Prevalence of multi-listing hosts: more than half of all listings owned by multi-listing hosts, indicating that multi-listing is common in london.
2. Professional landlords: hosts who owned 10+ listings owned more than one fifth listings, suggesting a significant presence of professional landlords in the market.

#### 6.1.2 Changes in the number of landlords and renters over the years

```{python}
# --- Count the number of new host IDs added each year ---
new_hosts_per_year = gdf.drop_duplicates('host_id').groupby('year').size()

# --- Count the cumulative number of host IDs each year ---
year_range = range(int(gdf['year'].min()), int(gdf['year'].max()) + 1)
id_per_year = [gdf[gdf['year'] <= year].shape[0] for year in year_range]

# Create a DataFrame to display results
result_df = pd.DataFrame({
    'Year': year_range,
    'New Hosts': new_hosts_per_year.reindex(year_range, fill_value=0),
    'Total Listings Count': id_per_year
})

# Plotting the data
fig, ax1 = plt.subplots(figsize=(20, 12))  # Greatly increased figure size

# --- Plot the number of new hosts added each year (bar chart) ---
bars = ax1.bar(result_df['Year'], result_df['New Hosts'], color='#6baed6', label='New Hosts per Year', alpha=0.8)
ax1.set_xlabel('Year', fontsize=24, fontweight='bold')  # Significantly larger font size
ax1.set_ylabel('New Hosts', color='#6baed6', fontsize=24, fontweight='bold')
ax1.tick_params(axis='y', labelcolor='#6baed6', labelsize=22)
ax1.tick_params(axis='x', labelsize=22)

# Annotate bar heights with values
for bar in bars:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width() / 2, height, f'{int(height)}', 
             ha='center', va='bottom', fontsize=18, color='#6baed6')  # Slightly smaller font for annotations

# --- Add a second y-axis for cumulative listings (line chart) ---
ax2 = ax1.twinx()
lines = ax2.plot(result_df['Year'], result_df['Total Listings Count'], color='#fd8d3c', marker='o', markersize=10, label='Total Listings Count')
ax2.set_ylabel('Listings Count', color='#e6550d', fontsize=24, fontweight='bold')
ax2.tick_params(axis='y', labelcolor='#e6550d', labelsize=22)

# Annotate line chart points with values
for x, y in zip(result_df['Year'], result_df['Total Listings Count']):
    ax2.text(x, y + 500, f'{int(y)}', ha='center', va='bottom', fontsize=18, color='#e6550d')  # Slightly smaller font for annotations

# --- Set x-axis ticks to display years as integers ---
plt.xticks(ticks=result_df['Year'], labels=result_df['Year'], rotation=45, fontsize=22)

# --- Add a legend ---
fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.95), fontsize=20)

# --- Set the title of the plot ---
plt.title('Yearly New Hosts and Listings on Airbnb in London', fontsize=30, fontweight='bold')

# --- Display the plot ---
plt.tight_layout()
plt.show()
```

* Based on Airbnb's dataset for London, the whole story started in 2008 and the growth in hosts and listings was slow between 2008 and 2010, **accelerating sharply from 2011 to 2016**. The peak occurred in 2015, with 8,939 new hosts, while 2014 and 2016 saw increases of over 7,000 hosts each. By 2016, total listings neared 50,000. 

* However, **growth slowed in subsequent years**, with 2020 and 2021 adding only around 1,600 hosts and 3,000 listings annually—nearly half the growth seen in 2019—largely due to the pandemic’s impact on the rental market. From 2022 to 2024, post-pandemic recovery is evident, but growth remains far below peak levels.

### 6.2 Analysis of property
#### 6.2.1 Distribution of room types of property

```{python}
#| output: false

# Count the number of each property type
property_counts = gdf['property_type'].value_counts()

# Filter out the categories with frequency greater than 200
filtered_property_counts = property_counts[property_counts > 200]

# Calculate the percentage for each category
total_count = property_counts.sum()  # 总的数量
filtered_percentage = (filtered_property_counts / total_count) * 100

# Draw a bar chart
plt.figure(figsize=(10, 6))
bars = filtered_property_counts.plot(kind='bar', color='skyblue')

# Add title and tag
plt.title('Distribution of Property Types (Count > 200)')
plt.xlabel('Property Type')
plt.ylabel('Count')

for bar, count, percentage in zip(bars.patches, filtered_property_counts, filtered_percentage):
    height = bar.get_height()
    # Show count at the top and percentage at the bottom
    plt.text(bar.get_x() + bar.get_width() / 2, height + 10, f'{count}', 
             ha='center', va='bottom', fontsize=9)
    plt.text(bar.get_x() + bar.get_width() / 2, height - 10, f'({percentage:.1f}%)', 
             ha='center', va='top', fontsize=9)

plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()
```

```{python}
#| output: false

room_type_counts = gdf['room_type'].value_counts()
room_type_percentage = gdf['room_type'].value_counts(normalize=True) * 100

room_type_stats = pd.DataFrame({
    'Count': room_type_counts,
    'Percentage': room_type_percentage
})

room_type_stats
```

***Room type of property*** is divided into four categories.

- Entire home/apt: 63.8%
- Private room: 35.6%
- Shared room: 0.45%
- Hotel room: 0.2%

**Conclusion:**

1. The high proportion of entire homes/apt indicates that many guests prefer independent accommodations for greater privacy and autonomy.  
    This aligns with a broader shift in tourism, where more visitors are opting for alternative lodging options instead of traditional hotels to enjoy a more spacious and private environment [@zervas2017rise].
2. The 35.6% share of private rooms suggests that some guests are still willing to choose more affordable accommodations, even if it means sharing common spaces. These listings cater to budget-conscious travelers.
3. The low percentages of shared rooms and hotel rooms indicate that Airbnb's core market in London, a well-established market, tends to favor more private lodging options.

#### 6.2.2 Distribution of Minimum Nights for renting property

```{python}
#| output: false

# Create a subplot
fig, ax = plt.subplots(figsize=(8, 4))  # Single subplot

# Set values greater than 35 to 35
gdf['minimum_nights_clipped'] = np.where(gdf['minimum_nights'] > 35, 35, gdf['minimum_nights'])
bins = list(range(1, 37))  # Bins from 1 to 35, with 35+ combined into one bin

# Plot the histogram for 'minimum_nights' column
ax.hist(gdf['minimum_nights_clipped'], bins=bins, color='lightblue', edgecolor='black')

# Add a vertical line for the threshold (at 30 days)
ax.axvline(x=30, color='black', linestyle='--', linewidth=1.5, label='STR Threshold (30 days)')

# Calculate the proportion below the 30-day threshold
threshold = 30
below_threshold = gdf[gdf['minimum_nights'] < threshold].shape[0]  # Count of listings below 30 days
total_entries = gdf.shape[0]  # Total count of listings
percentage_below = (below_threshold / total_entries) * 100

# Display the percentage information on the graph
ax.text(18, ax.get_ylim()[1] * 0.8,  # Set position
        f'{below_threshold} listings ({percentage_below:.1f}%) below 30 nights',
        fontsize=12, color='black', ha='center', bbox=dict(facecolor='white', alpha=0.6))

# Set the title and labels
ax.set_title('Distribution of Minimum Nights', fontsize=15)
ax.set_xlabel('Minimum Nights')
ax.set_ylabel('Count')
ax.legend()

# Set x-axis ticks and labels
x_ticks = list(range(1, 8)) + list(range(10, 36, 5))  # Show daily ticks for the first 7 days, and every 5 days from 10 to 35
x_labels = [str(i) for i in x_ticks[:-1]] + ['35+']  # Define tick labels
ax.set_xticks(x_ticks)
ax.set_xticklabels(x_labels)

# Display the plot
plt.tight_layout()
plt.show()
```

```{python}
#| output: false

#Percentage of listings for which minimum nights are less than 7.
gdf[gdf['minimum_nights']<=7].shape[0] / total_entries # 0.9232768749285172
```

Based on the dataset, 93550 listings have a minimum night stay of less than the STR threshold (30 days), making up 97.3% of the total. Additionally, listings with a minimum stay of less than 7 days account for 92.3% of the total. 
**The London rental market on airbnb is dominated by short-term rentals.**

Chaudhary had illustrated some drawbacks of short term renting [@chaudhary2021effects].

1. **Reduced long-term housing supply**: Due to higher profits from short-term rentals (e.g., Airbnb), many landlords prioritize short-term leases over long-term rentals, exacerbating London's housing crisis and driving up rents, especially for low- and middle-income residents.
2. **Community impacts**: A high volume of short-term rentals can disrupt neighborhoods, increasing noise and tourist traffic, making communities less appealing for long-term residents and undermining stability and safety.


## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London? 

::: {.duedate}

( 45 points; Answer due {{< var assess.group-date >}} )

:::


## Sustainable Authorship Tools

Using the Terminal in Docker, you compile the Quarto report using `quarto render <group_submission_file>.qmd`.

Your QMD file should automatically download your BibTeX and CLS files and any other required files. If this is done right after library loading then the entire report should output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References
